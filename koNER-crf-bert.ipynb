{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-crf in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (0.7.2)\n",
      "Requirement already satisfied: ftfy in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (6.1.1)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from ftfy) (0.2.5)\r\n",
      "Collecting transformers==3\r\n",
      "  Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\r\n",
      "Requirement already satisfied: packaging in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from transformers==3) (22.0)\r\n",
      "Requirement already satisfied: numpy in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from transformers==3) (1.21.5)\r\n",
      "Collecting tokenizers==0.8.0-rc4\r\n",
      "  Using cached tokenizers-0.8.0rc4.tar.gz (96 kB)\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting sacremoses\r\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\r\n",
      "Requirement already satisfied: sentencepiece in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from transformers==3) (0.1.95)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from transformers==3) (4.64.1)\r\n",
      "Requirement already satisfied: requests in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from transformers==3) (2.28.1)\r\n",
      "Requirement already satisfied: filelock in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from transformers==3) (3.9.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from transformers==3) (2022.7.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from requests->transformers==3) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from requests->transformers==3) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from requests->transformers==3) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from requests->transformers==3) (1.26.14)\r\n",
      "Requirement already satisfied: six in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from sacremoses->transformers==3) (1.16.0)\r\n",
      "Requirement already satisfied: click in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from sacremoses->transformers==3) (8.0.4)\r\n",
      "Requirement already satisfied: joblib in /home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages (from sacremoses->transformers==3) (1.1.1)\r\n",
      "Building wheels for collected packages: tokenizers\r\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l-\b \berror\r\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\r\n",
      "  \r\n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\r\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\r\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[48 lines of output]\u001b[0m\r\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-t7jz30yb/overlay/lib/python3.9/site-packages/setuptools/dist.py:534: UserWarning: Normalizing '0.8.0.rc4' to '0.8.0rc4'\r\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(tmpl.format(**locals()))\r\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\r\n",
      "  \u001b[31m   \u001b[0m running build\r\n",
      "  \u001b[31m   \u001b[0m running build_py\r\n",
      "  \u001b[31m   \u001b[0m creating build\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/tokenizers\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/tokenizers/models\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/models\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/tokenizers/decoders\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/decoders\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/tokenizers/normalizers\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/normalizers\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/tokenizers/pre_tokenizers\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/pre_tokenizers\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/tokenizers/processors\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/processors\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/tokenizers/trainers\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/trainers\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/tokenizers/implementations\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/models\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/decoders\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/normalizers\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/pre_tokenizers\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/processors\r\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/trainers\r\n",
      "  \u001b[31m   \u001b[0m running build_ext\r\n",
      "  \u001b[31m   \u001b[0m running build_rust\r\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\r\n",
      "  \u001b[31m   \u001b[0m \r\n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\r\n",
      "  \u001b[31m   \u001b[0m \r\n",
      "  \u001b[31m   \u001b[0m To update pip, run:\r\n",
      "  \u001b[31m   \u001b[0m \r\n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\r\n",
      "  \u001b[31m   \u001b[0m \r\n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\r\n",
      "  \u001b[31m   \u001b[0m \r\n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\r\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\r\n",
      "  \r\n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\r\n",
      "\u001b[0mFailed to build tokenizers\r\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "!pip install pytorch-crf\n",
    "!pip install ftfy\n",
    "!pip install transformers==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule, BertTokenizer, BertModel, XLMRobertaConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from torchcrf import CRF\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "MAX_TOKEN = 180 ## 80 for the naver format, 250 for the conll format\n",
    "TRAIN_BATCH_SIZE = 60\n",
    "VALID_BATCH_SIZE = 60\n",
    "EPOCHS = 200\n",
    "LOAD_MODEL = False\n",
    "BERT_MODEL = 'bert-base-multilingual-uncased'\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "#TRAIN_FILE = \"./data/train_data_annotated_BIOES_v3.txt\"\n",
    "#VALID_FILE = \"./data/e\"\n",
    "#TRAIN_FILE = \"./data/xa\"\n",
    "#TRAIN_FILE = \"./data/train_data.bio.converted\"\n",
    "#VALID_FILE = \"./data/train_data.bio.converted\"\n",
    "TRAIN_FILE = \"./data/bio_group1_test.conllu\"\n",
    "VALID_FILE = \"./data/bio_group1_test.conllu\"\n",
    "\n",
    "#TRAIN_FILE = \"./data/xa_naverToCoNLL\"\n",
    "#VALID_FILE = \"./data/xa\"\n",
    "#TRAIN_FILE = \"/home/ktlim/code/TTtagger/corpus/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-train.conllu\"\n",
    "#VALID_FILE = \"/home/ktlim/code/TTtagger/corpus/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=0\n",
    "ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(word):\n",
    "    w = re.sub(r\".+@.+\", \"*EMAIL*\", word)\n",
    "    w = re.sub(r\"@\\w+\", \"*AT*\", w)\n",
    "    w = re.sub(r\"(https?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"(http?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"(HTTPS?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"(HTTP?://|www\\.).*\", \"*url*\", w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def strong_normalize(word):\n",
    "    w = ftfy.fix_text(word.lower())\n",
    "    w = re.sub(r\".+@.+\", \"*EMAIL*\", w)\n",
    "    w = re.sub(r\"@\\w+\", \"*AT*\", w)\n",
    "    w = re.sub(r\"(https?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"(http?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"([^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"([^\\d][^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"``\", '\"', w)\n",
    "    w = re.sub(r\"''\", '\"', w)\n",
    "    w = re.sub(r\"\\d\", \"0\", w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def buildVocab(graphs, cutoff=1):\n",
    "    wordsCount = Counter()\n",
    "    charsCount = Counter()\n",
    "    uposCount = Counter()\n",
    "    xposCount = Counter()\n",
    "    relCount = Counter()\n",
    "    featCount = Counter()\n",
    "    langCount = Counter()\n",
    "\n",
    "    for graph in graphs:\n",
    "        wordsCount.update([node.norm for node in graph.nodes[1:]])\n",
    "        for node in graph.nodes[1:]:\n",
    "            charsCount.update(list(node.word))\n",
    "            featCount.update(node.feats_set)\n",
    "            #  charsCount.update(list(node.norm))\n",
    "        uposCount.update([node.upos for node in graph.nodes[1:]])\n",
    "        xposCount.update([node.xupos for node in graph.nodes[1:]])\n",
    "        relCount.update([rel for rel in graph.rels[1:]])\n",
    "        langCount.update([node.lang for node in graph.nodes[1:]])\n",
    "        \n",
    "\n",
    "    wordsCount = Counter({w: i for w, i in wordsCount.items() if i >= cutoff})\n",
    "    print(\"Vocab containing {} words\".format(len(wordsCount)))\n",
    "    print(\"Charset containing {} chars\".format(len(charsCount)))\n",
    "    print(\"UPOS containing {} tags\".format(len(uposCount)), uposCount)\n",
    "    #print(\"XPOS containing {} tags\".format(len(xposCount)), xposCount)\n",
    "    print(\"Rels containing {} tags\".format(len(relCount)), relCount)\n",
    "    print(\"Feats containing {} tags\".format(len(featCount)), featCount)\n",
    "    print(\"lang containing {} tags\".format(len(langCount)), langCount)\n",
    "\n",
    "    ret = {\n",
    "        \"vocab\": list(wordsCount.keys()),\n",
    "        \"wordfreq\": wordsCount,\n",
    "        \"charset\": list(charsCount.keys()),\n",
    "        \"charfreq\": charsCount,\n",
    "        \"upos\": list(uposCount.keys()),\n",
    "        \"xpos\": list(xposCount.keys()),\n",
    "        \"rels\": list(relCount.keys()),\n",
    "        \"feats\": list(featCount.keys()),\n",
    "        \"lang\": list(langCount.keys()),\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def shuffled_stream(data):\n",
    "    len_data = len(data)\n",
    "    while True:\n",
    "        for d in random.sample(data, len_data):\n",
    "            yield d\n",
    "\n",
    "def shuffled_balanced_stream(data):\n",
    "    for ds in zip(*[shuffled_stream(s) for s in data]):\n",
    "        ds = list(ds)\n",
    "        random.shuffle(ds)\n",
    "        for d in ds:\n",
    "            yield d\n",
    "            \n",
    "            \n",
    "def parse_dict(features):\n",
    "    if features is None or features == \"_\":\n",
    "        return {}\n",
    "\n",
    "    ret = {}\n",
    "    lst = features.split(\"|\")\n",
    "    for l in lst:\n",
    "        k, v = l.split(\"=\")\n",
    "        ret[k] = v\n",
    "    return ret\n",
    "\n",
    "\n",
    "def parse_features(features):\n",
    "    if features is None or features == \"_\":\n",
    "        return set()\n",
    "\n",
    "    return features.lower().split(\"|\")\n",
    "\n",
    "\n",
    "class Word:\n",
    "\n",
    "    def __init__(self, word, upos, lemma=None, xpos=None, feats=None, misc=None, lang=None):\n",
    "        self.word = normalize(word)\n",
    "        self.norm = strong_normalize(word) #strong_normalize(word)\n",
    "        self.lemma = lemma if lemma else \"_\"\n",
    "        self.upos = upos\n",
    "        self.xpos = xpos if xpos else \"_\"\n",
    "        self.xupos = self.upos + \"|\" + self.xpos\n",
    "        self.feats = feats if feats else \"_\"\n",
    "        self.feats_set = parse_features(self.feats)\n",
    "        self.misc = misc if misc else \"_\"\n",
    "        self.lang = lang if lang else \"_\"\n",
    "\n",
    "    def cleaned(self):\n",
    "        return Word(self.word, \"_\")\n",
    "\n",
    "    def clone(self):\n",
    "        return Word(self.word, self.upos, self.lemma, self.xpos, self.feats, self.misc)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}_{}\".format(self.word, self.upos)\n",
    "\n",
    "\n",
    "class DependencyGraph(object):\n",
    "\n",
    "    def __init__(self, words, tokens=None):\n",
    "        #  Token is a tuple (start, end, form)\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        self.nodes = np.array([Word(\"*root*\", \"*root*\")] + list(words))\n",
    "        self.tokens = tokens\n",
    "        self.heads = np.array([-1] * len(self.nodes))\n",
    "        self.rels = np.array([\"_\"] * len(self.nodes), dtype=object)\n",
    "\n",
    "    def __copy__(self):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        result.nodes = self.nodes\n",
    "        result.tokens = self.tokens\n",
    "        result.heads = self.heads.copy()\n",
    "        result.rels = self.rels.copy()\n",
    "        return result\n",
    "\n",
    "    def cleaned(self, node_level=True):\n",
    "        if node_level:\n",
    "            return DependencyGraph([node.cleaned() for node in self.nodes[1:]], self.tokens)\n",
    "        else:\n",
    "            return DependencyGraph([node.clone() for node in self.nodes[1:]], self.tokens)\n",
    "\n",
    "    def attach(self, head, tail, rel):\n",
    "        self.heads[tail] = head\n",
    "        self.rels[tail] = rel\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([\"{} ->({})  {} ({})\".format(str(self.nodes[i]), self.rels[i], self.heads[i], self.nodes[self.heads[i]]) for i in range(len(self.nodes))])\n",
    "\n",
    "\n",
    "def read_conll(filename, lang_code=None, max_token=80):\n",
    "    \n",
    "    print(\"read_conll with\", lang_code)\n",
    "    def get_word(columns):\n",
    "        return Word(columns[FORM], columns[UPOS], lemma=columns[LEMMA], xpos=columns[XPOS], feats=columns[FEATS], misc=columns[MISC], lang=lang_code)\n",
    "\n",
    "    def get_graph(graphs, words, tokens, edges):\n",
    "        graph = DependencyGraph(words, tokens)\n",
    "        for (h, d, r) in edges:\n",
    "            graph.attach(h, d, r)\n",
    "        graphs.append(graph)\n",
    "\n",
    "    file = open(filename, \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "    graphs = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    edges = []\n",
    "\n",
    "    num_sent = 0\n",
    "    sentence_start = False\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            if len(words) > 0:\n",
    "                get_graph(graphs, words, tokens, edges)\n",
    "                words, tokens, edges = [], [], []\n",
    "            break\n",
    "        line = line.rstrip(\"\\r\\n\")\n",
    "\n",
    "        # Handle sentence start boundaries\n",
    "        if not sentence_start:\n",
    "            # Skip comments\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            # Start a new sentence\n",
    "            sentence_start = True\n",
    "        if not line:\n",
    "            sentence_start = False\n",
    "            if len(words) > 0:\n",
    "                if (len(words) < max_token):\n",
    "                    get_graph(graphs, words, tokens, edges)\n",
    "                words, tokens, edges = [], [], []\n",
    "                num_sent += 1\n",
    "            continue\n",
    "\n",
    "        # Read next token/word\n",
    "        columns = line.split(\"\\t\")\n",
    "\n",
    "        # Skip empty nodes\n",
    "        if \".\" in columns[ID]:\n",
    "            continue\n",
    "\n",
    "        # Handle multi-word tokens to save word(s)\n",
    "        if \"-\" in columns[ID]:\n",
    "            start, end = map(int, columns[ID].split(\"-\"))\n",
    "            tokens.append((start, end + 1, columns[FORM]))\n",
    "\n",
    "            for _ in range(start, end + 1):\n",
    "                word_line = file.readline().rstrip(\"\\r\\n\")\n",
    "                word_columns = word_line.split(\"\\t\")\n",
    "                #LKT added for making a limitation for the word length to 30!!!\n",
    "                #if len(get_word(word_columns).word) >30:\n",
    "                    #print(get_word(word_columns))\n",
    "                words.append(get_word(word_columns))\n",
    "                if word_columns[HEAD].isdigit():\n",
    "                    head = int(word_columns[HEAD])\n",
    "                else:\n",
    "                    head = -1\n",
    "                edges.append((head, int(word_columns[ID]), word_columns[DEPREL].split(\":\")[0]))\n",
    "        # Basic tokens/words\n",
    "        else:\n",
    "            #LKT added for making a limitation for the word length to 30!!!\n",
    "            #if len(get_word(columns).word) > 30:\n",
    "                #print(get_word(columns))\n",
    "            words.append(get_word(columns))\n",
    "            if columns[HEAD].isdigit():\n",
    "                head = int(columns[HEAD])\n",
    "            else:\n",
    "                head = -1\n",
    "            edges.append((head, int(columns[ID]), columns[DEPREL].split(\":\")[0]))\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def naver_to_conll(read_file, write_file):\n",
    "    with open(WRITE_FILE,\"w\", encoding=\"UTF-8\") as write_file:\n",
    "        with open(READ_FILE, \"r\", encoding=\"UTF-8\") as read_file:\n",
    "            for line in read_file.readlines():\n",
    "                if line.startswith(\"# sent_id = \"):\n",
    "                    write_file.writelines('\\n')\n",
    "                    write_file.writelines(line)\n",
    "                elif line.startswith(\"# ner = \") or line.startswith(\"# text = \"):\n",
    "                    write_file.writelines(line)\n",
    "\n",
    "                if line.startswith(\"# ner = \"):\n",
    "                    trim_line = line.replace(\"# ner = \", \"\").replace('\\n', \"\")\n",
    "                    tokens = trim_line.split(' ')\n",
    "                    token_idx = 1\n",
    "                    for token in tokens:\n",
    "                        out = token.split('/')\n",
    "                        word, tag = out[0], out[1]\n",
    "                        a_line = str(token_idx)+'\\t'+str(word)+'\\t'+str(word)+'\\t'+str('NOUN')+'\\t'+str('NNG')+'\\t'+str(tag).replace('-','O')+'\\t'+str('_')+'\\t'+str('_')+'\\t'+str('_')+'\\t'+str('_')+'\\n'\n",
    "                        token_idx = token_idx+1 \n",
    "                        write_file.writelines(a_line)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loader\n",
    "class CoNLLDataset:\n",
    "    def __init__(self, graphs, tokenizer, max_len, fullvocab=None):\n",
    "        self.conll_graphs = graphs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self._fullvocab = fullvocab if fullvocab else buildVocab(self.conll_graphs, cutoff=1)\n",
    "            \n",
    "        self._upos = {p: i for i, p in enumerate(self._fullvocab[\"upos\"])}\n",
    "        self._iupos = self._fullvocab[\"upos\"]\n",
    "        self._xpos = {p: i for i, p in enumerate(self._fullvocab[\"xpos\"])}\n",
    "        self._ixpos = self._fullvocab[\"xpos\"]\n",
    "        self._vocab = {w: i+3 for i, w in enumerate(self._fullvocab[\"vocab\"])}\n",
    "        self._wordfreq = self._fullvocab[\"wordfreq\"]\n",
    "        self._charset = {c: i+3 for i, c in enumerate(self._fullvocab[\"charset\"])}\n",
    "        self._charfreq = self._fullvocab[\"charfreq\"]\n",
    "        self._rels = {r: i for i, r in enumerate(self._fullvocab[\"rels\"])}\n",
    "        self._irels = self._fullvocab[\"rels\"]\n",
    "        self._feats = {f: i for i, f in enumerate(self._fullvocab[\"feats\"])}\n",
    "        self._langs = {r: i+2 for i, r in enumerate(self._fullvocab[\"lang\"])}\n",
    "        self._ilangs = self._fullvocab[\"lang\"]\n",
    "        \n",
    "        #self._posRels = {r: i for i, r in enumerate(self._fullvocab[\"posRel\"])}\n",
    "        #self._iposRels = self._fullvocab[\"posRel\"]\n",
    "        \n",
    "        self._vocab['*pad*'] = 0\n",
    "        self._charset['*pad*'] = 0\n",
    "        self._langs['*pad*'] = 0\n",
    "        \n",
    "        self._vocab['*root*'] = 1\n",
    "        self._charset['*whitespace*'] = 1\n",
    "        \n",
    "        self._vocab['*unknown*'] = 2\n",
    "        self._charset['*unknown*'] = 2\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.conll_graphs)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        graph = self.conll_graphs[item]\n",
    "        word_list = [node.word for node in graph.nodes]\n",
    "        upos_list = [node.upos for node in graph.nodes]\n",
    "        feat_list = [node.feats for node in graph.nodes]\n",
    "        \n",
    "        encoded = self.tokenizer.encode_plus(' '.join(word_list[1:]),\n",
    "                                             None,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length = self.max_len,\n",
    "                                             truncation=True,\n",
    "                                             pad_to_max_length = True)\n",
    "        \n",
    "        ids, mask = encoded['input_ids'], encoded['attention_mask']\n",
    "        \n",
    "        bpe_head_mask = [0]; upos_ids = [-1]; feat_ids = [-1] # --> CLS token\n",
    "        \n",
    "        for word, upos, feat in zip(word_list[1:], upos_list[1:], feat_list[1:]):\n",
    "            bpe_len = len(self.tokenizer.tokenize(word))\n",
    "            head_mask = [1] + [0]*(bpe_len-1)\n",
    "            bpe_head_mask.extend(head_mask)\n",
    "            upos_mask = [self._upos.get(upos, 0)] + [-1]*(bpe_len-1)\n",
    "            upos_ids.extend(upos_mask)\n",
    "            feat_mask = [self._feats.get(feat.lower(), 2)] + [-1]*(bpe_len-1)\n",
    "            feat_ids.extend(feat_mask)\n",
    "            \n",
    "            #print(\"head_mask\", head_mask)\n",
    "        \n",
    "        bpe_head_mask.append(0); upos_ids.append(-1); feat_ids.append(-1) # --> END token\n",
    "        bpe_head_mask.extend([0] * (self.max_len - len(bpe_head_mask))) ## --> padding by max_len\n",
    "        upos_ids.extend([-1] * (self.max_len - len(upos_ids))) ## --> padding by max_len\n",
    "        feat_ids.extend([-1] * (self.max_len - len(feat_ids))) ## --> padding by max_len\n",
    "        \n",
    "        return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'bpe_head_mask': torch.tensor(bpe_head_mask, dtype=torch.long),\n",
    "                'upos_ids': torch.tensor(upos_ids, dtype=torch.long),\n",
    "                'feat_ids': torch.tensor(feat_ids, dtype=torch.long)\n",
    "               }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(total_pred, total_targ, noNER_idx):\n",
    "    \n",
    "    p = 0 # (retrived SB and real SB) / retrived SB  # The percentage of (the number of correct predictions) / (the number of predction that system predicts as B-SENT)\n",
    "    r = 0\n",
    "    f1= 0\n",
    "    \n",
    "    np_total_pred = np.array(total_pred)\n",
    "    np_total_tag = np.array(total_targ)\n",
    "    \n",
    "    #Get noPad\n",
    "    incidence_nopad = np.where(np_total_tag != -1) ## eliminate paddings\n",
    "    np_total_pred_nopad = np_total_pred[incidence_nopad]\n",
    "    np_total_tag_nopad = np_total_tag[incidence_nopad]\n",
    "    \n",
    "    \n",
    "    #precision\n",
    "    incidence_nopad_sb = np.where(np_total_pred_nopad != noNER_idx)\n",
    "    np_total_pred_nopad_sb = np_total_pred_nopad[incidence_nopad_sb]\n",
    "    np_total_tag_nopad_sb = np_total_tag_nopad[incidence_nopad_sb]\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_nopad_sb)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_nopad_sb==np_total_tag_nopad_sb) == True)\n",
    "    \n",
    "    '''\n",
    "    np_total_pred_incid = np_total_pred[incidence_p]\n",
    "    print(\"np_total_pred_incid\", np_total_pred_incid)\n",
    "    ids_sb_pred_p = np.where(np_total_pred_incid==1)\n",
    "    np_total_pred_p = np_total_pred_incid[ids_sb_pred_p]\n",
    "    np_total_tag_p = np_total_tag[ids_sb_pred_p]\n",
    "    \n",
    "    print(\"ids_sb_pred_p\", ids_sb_pred_p)\n",
    "    print(\"np_total_pred_p\", np_total_pred_p)\n",
    "    print(\"np_total_tag_p\", np_total_tag_p)\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_p)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_p==np_total_tag_p) == True)\n",
    "    '''\n",
    "    \n",
    "    print(\"count_correct_p\", count_correct_p)\n",
    "    print(\"count_active_tokens_p\", count_active_tokens_p)\n",
    "    \n",
    "    p = count_correct_p/count_active_tokens_p\n",
    "    print(\"precision:\", p)\n",
    "\n",
    "    \n",
    "    #recall\n",
    "    ids_sb_pred_r = np.where(np_total_tag_nopad != noNER_idx)\n",
    "    np_total_pred_r = np_total_pred_nopad[ids_sb_pred_r]\n",
    "    np_total_tag_r = np_total_tag_nopad[ids_sb_pred_r]\n",
    "    \n",
    "    #print(\"ids_sb_pred_r\", ids_sb_pred_r)\n",
    "    #print(\"np_total_pred_r\", np_total_pred_r)\n",
    "    #print(\"np_total_tag_r\", np_total_tag_r)\n",
    "    \n",
    "    count_active_tokens_r = len(np_total_pred_r)\n",
    "    count_correct_r = np.count_nonzero((np_total_pred_r==np_total_tag_r) == True)\n",
    "    \n",
    "    print(\"count_active_tokens_r\", count_active_tokens_r)\n",
    "    print(\"count_correct_r\", count_correct_r)\n",
    "    \n",
    "    r = count_correct_r/count_active_tokens_r\n",
    "    print(\"recall:\", r)\n",
    "    \n",
    "    \n",
    "    #F1\n",
    "    #f1 = 2*(p*r) / (p+r)\n",
    "    print(\"F1:\", f1)\n",
    "    \n",
    "    #count_active_tokens_recall = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    #print(\"count_active_tokens_recall\", count_active_tokens_recall)\n",
    "    #count_active_tokens_precision = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    \n",
    "    #count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"count_correct\",count_correct)\n",
    "    #print(\"ACCURACY:\", count_correct/count_active_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaEncoder(nn.Module):\n",
    "    def __init__(self, num_upos, num_feat):\n",
    "        super(XLMRobertaEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size =768\n",
    "        self.xlm_roberta = transformers.BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear = nn.Linear(self.hidden_size, num_upos)\n",
    "        \n",
    "        self.f_dropout = nn.Dropout(0.33)\n",
    "        self.f_linear = nn.Linear(768, num_feat)\n",
    "        \n",
    "        # Take the CRF module at https://github.com/eagle705/pytorch-bert-crf-ner\n",
    "        self.crf = CRF(num_tags=num_feat, batch_first=True)\n",
    "        \n",
    "            \n",
    "    def forward(self, ids, mask, targets=None):\n",
    "        o1, o2 = self.xlm_roberta(ids, mask)\n",
    "        \n",
    "        #apool = torch.mean(o1, 1)\n",
    "        #mpool, _ = torch.max(o1, 1)\n",
    "        #cat = torch.cat((apool, mpool), 1)\n",
    "        #bo = self.dropout(cat)\n",
    "        \n",
    "        o1 = self.f_dropout(o1)\n",
    "        p_logits = self.linear(o1)        \n",
    "        f_logits = self.f_linear(o1)   \n",
    "        \n",
    "        \n",
    "        if targets is not None:\n",
    "            log_likelihood, crf_out = self.crf(f_logits, targets), self.crf.decode(f_logits)\n",
    "            return p_logits, f_logits, log_likelihood, crf_out #since the log_likelihood is a number, it is not working on multi-gpu, probably it should be a tensor\n",
    "        else:\n",
    "            crf_out = self.crf.decode(f_logits)\n",
    "            return p_logits, f_logits, crf_out\n",
    "        \n",
    "        #print(crf_out, crf_out.size)\n",
    "        \n",
    "        \n",
    "        #if tags is not None: # crf training\n",
    "        #    log_likelihood, sequence_of_tags = self.crf(emissions, tags), self.crf.decode(emissions)\n",
    "        #    return log_likelihood, sequence_of_tags\n",
    "        #else: # tag inference\n",
    "        #    sequence_of_tags = self.crf.decode(emissions)\n",
    "        #    return sequence_of_tags, outputs\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_conll with ko\n"
     ]
    }
   ],
   "source": [
    "train_graphs = read_conll(TRAIN_FILE, 'ko', MAX_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*root*_*root* ->(_)  -1 (．_PUNCT)\n",
       "(_PUNCT ->(rel)  5 (．_PUNCT)\n",
       "삼성_PROPN ->(rel)  5 (．_PUNCT)\n",
       "멜시토프_PROPN ->(rel)  5 (．_PUNCT)\n",
       "관할_NOUN ->(rel)  5 (．_PUNCT)\n",
       "．_PUNCT ->(root)  0 (*root*_*root*)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['_', 'rel', 'rel', 'rel', 'rel', 'root'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_sample = train_graphs[0]\n",
    "a_sample.rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_conll with ko\n",
      "Vocab containing 27882 words\n",
      "Charset containing 1633 chars\n",
      "UPOS containing 15 tags Counter({'NOUN': 68360, 'PRT': 47625, 'ADP': 35090, 'PUNCT': 21412, 'VERB': 21176, 'PROPN': 9385, 'NUM': 8807, 'X': 3650, 'ADJ': 2872, 'DET': 2778, 'SYM': 1516, 'PRON': 1476, 'ADV': 1097, 'INTJ': 131, 'CCONJ': 28})\n",
      "Rels containing 2 tags Counter({'rel': 216404, 'root': 8999})\n",
      "Feats containing 29 tags Counter({'o': 171098, 'num_i': 7784, 'cvl_b': 5871, 'num_b': 5740, 'per_b': 4306, 'org_b': 4114, 'dat_i': 3634, 'org_i': 3475, 'dat_b': 2636, 'trm_i': 2378, 'per_i': 2122, 'loc_b': 2104, 'evt_i': 1979, 'cvl_i': 1973, 'trm_b': 1880, 'evt_b': 1093, 'anm_b': 636, 'afw_i': 489, 'loc_i': 472, 'tim_i': 461, 'afw_b': 404, 'tim_b': 344, 'fld_b': 244, 'fld_i': 62, 'plt_b': 41, 'anm_i': 36, 'mat_b': 19, 'plt_i': 6, 'mat_i': 2})\n",
      "lang containing 1 tags Counter({'ko': 225403})\n",
      "read_conll with ko\n"
     ]
    }
   ],
   "source": [
    "train_graphs = read_conll(TRAIN_FILE, 'ko', MAX_TOKEN)\n",
    "train_dataset = CoNLLDataset(graphs=train_graphs, tokenizer=TOKENIZER, max_len=MAX_LEN)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=4, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "valid_graphs = read_conll(VALID_FILE, 'ko', MAX_TOKEN)\n",
    "valid_dataset = CoNLLDataset(graphs=valid_graphs, tokenizer=TOKENIZER, max_len=MAX_LEN, fullvocab=train_dataset._fullvocab)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=4, batch_size=VALID_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "num_upos = len(train_dataset._upos)\n",
    "num_feat = len(train_dataset._feats)\n",
    "model = XLMRobertaEncoder(num_upos, num_feat)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "lr = 0.000005\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(train_loader, model, optimizer, DEVICE, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    p_total_pred = []\n",
    "    p_total_targ = []\n",
    "    p_total_loss = []\n",
    "    \n",
    "    f_total_pred = []\n",
    "    f_total_targ = []\n",
    "    f_total_loss = []\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        p_logits, f_logits, likelihood, crf_out = model(batch['ids'].cuda(), batch['mask'].cuda(), batch['feat_ids'].cuda())\n",
    "        \n",
    "        #UPOS\n",
    "        b,s,l = p_logits.size()\n",
    "        #print(p_logits.view(b*s,l), p_logits.view(b*s,l).size())\n",
    "        #print(batch['upos_ids'].cuda().view(b*s), batch['upos_ids'].cuda().view(b*s).size())\n",
    "        p_loss = loss_fn(p_logits.view(b*s,l), batch['upos_ids'].cuda().view(b*s))\n",
    "        p_total_loss.append(p_loss.item())\n",
    "        p_total_pred.extend(torch.argmax(p_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        p_total_targ.extend(batch['upos_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        \n",
    "        #FEAT\n",
    "        b,s,l = f_logits.size()\n",
    "        f_loss = loss_fn(f_logits.view(b*s,l), batch['feat_ids'].cuda().view(b*s))\n",
    "        f_total_loss.append(f_loss.item())\n",
    "        #f_total_pred.extend(torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        \n",
    "        #f_total_pred.extend(crf_out)\n",
    "        f_total_targ.extend(batch['feat_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        \n",
    "        for s in crf_out:\n",
    "            f_total_pred.extend(s)\n",
    "        \n",
    "        #loss = p_loss + (-1 * likelihood)\n",
    "        loss = -1 * likelihood\n",
    "        #print(\"crf_out\", crf_out)\n",
    "        #print(\"pred\", torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    count_active_tokens = np.count_nonzero(np.array(p_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(p_total_pred)==np.array(p_total_targ)) == True)\n",
    "    print(\"TRAINING POS ACCURACY:\", count_correct/count_active_tokens)\n",
    "    \n",
    "    count_active_tokens = np.count_nonzero(np.array(f_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(f_total_pred)==np.array(f_total_targ)) == True)\n",
    "    f1_score(f_total_pred, f_total_targ, train_dataset._feats.get('o', 2))\n",
    "    print(\"TRAINING FEAT ACCURACY:\", count_correct/count_active_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loop_fn(dev_loader, model, DEVICE):\n",
    "    model.eval()\n",
    "    \n",
    "    p_total_pred = []\n",
    "    p_total_targ = []\n",
    "    p_total_loss = []\n",
    "    \n",
    "    f_total_pred = []\n",
    "    f_total_targ = []\n",
    "    f_total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(dev_loader), total=len(dev_loader)):\n",
    "\n",
    "            p_logits, f_logits, likelihood, crf_out = model(batch['ids'].cuda(), batch['mask'].cuda(), batch['feat_ids'].cuda())\n",
    "\n",
    "            #UPOS\n",
    "            b,s,l = p_logits.size()\n",
    "            #print(p_logits.view(b*s,l), p_logits.view(b*s,l).size())\n",
    "            #print(batch['upos_ids'].cuda().view(b*s), batch['upos_ids'].cuda().view(b*s).size())\n",
    "            p_loss = loss_fn(p_logits.view(b*s,l), batch['upos_ids'].cuda().view(b*s))\n",
    "            p_total_loss.append(p_loss.item())\n",
    "            p_total_pred.extend(torch.argmax(p_logits.view(b*s,l), 1).cpu().tolist())\n",
    "            p_total_targ.extend(batch['upos_ids'].cuda().view(b*s).cpu().tolist())\n",
    "\n",
    "            #FEAT\n",
    "            b,s,l = f_logits.size()\n",
    "            f_loss = loss_fn(f_logits.view(b*s,l), batch['feat_ids'].cuda().view(b*s))\n",
    "            f_total_loss.append(f_loss.item())\n",
    "            #f_total_pred.extend(torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "\n",
    "            #f_total_pred.extend(crf_out)\n",
    "            f_total_targ.extend(batch['feat_ids'].cuda().view(b*s).cpu().tolist())\n",
    "\n",
    "            for s in crf_out:\n",
    "                f_total_pred.extend(s)\n",
    "\n",
    "            loss = p_loss\n",
    "        \n",
    "        \n",
    "    count_active_tokens = np.count_nonzero(np.array(p_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(p_total_pred)==np.array(p_total_targ)) == True)\n",
    "    print(\"VALIDATION POS ACCURACY:\", count_correct/count_active_tokens)\n",
    "    \n",
    "    count_active_tokens = np.count_nonzero(np.array(f_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(f_total_pred)==np.array(f_total_targ)) == True)\n",
    "    f1_score(f_total_pred, f_total_targ, train_dataset._feats.get('o', 2))\n",
    "    print(\"VALIDATION FEAT ACCURACY:\", count_correct/count_active_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\r  0%|          | 0/150 [00:00<?, ?it/s]/home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/aailab_conda/anaconda3/envs/koner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\r  0%|          | 0/150 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dropout(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3592035/1978683380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mvalid_loop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3592035/2558881037.py\u001b[0m in \u001b[0;36mtrain_loop_fn\u001b[0;34m(train_loader, model, optimizer, DEVICE, scheduler)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mp_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feat_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#UPOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/koner/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/koner/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/koner/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3592035/1318499351.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, targets)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#bo = self.dropout(cat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mp_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mf_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/koner/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/koner/lib/python3.9/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/koner/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dropout(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "LOAD_MODEL=False\n",
    "step=0\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "    \n",
    "for idx in range(EPOCHS):\n",
    "    train_loop_fn(train_loader, model, optimizer, DEVICE)\n",
    "    valid_loop_fn(valid_loader, model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"step\": step,\n",
    "}\n",
    "save_checkpoint(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "59ab1b074a455bfe8e5b75eaed9c91d1b25b566c8aeea06dfe3c955be6e1459e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
