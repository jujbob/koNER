{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytorch-crf in /home/work/.local/lib/python3.6/site-packages (0.7.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ftfy in /home/work/.local/lib/python3.6/site-packages (5.8)\n",
      "Requirement already satisfied: wcwidth in /home/work/.local/lib/python3.6/site-packages (from ftfy) (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "!pip install pytorch-crf\n",
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule, BertTokenizer, BertModel, XLMRobertaConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from torchcrf import CRF\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "MAX_TOKEN = 180 ## 80 for the naver format, 250 for the conll format\n",
    "TRAIN_BATCH_SIZE = 18\n",
    "VALID_BATCH_SIZE = 12\n",
    "EPOCHS = 200\n",
    "LOAD_MODEL = False\n",
    "BERT_MODEL = 'bert-base-multilingual-uncased'\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "#TRAIN_FILE = \"./data/train_data_annotated_BIOES_v3.txt\"\n",
    "#VALID_FILE = \"./data/e\"\n",
    "#TRAIN_FILE = \"./data/xa\"\n",
    "TRAIN_FILE = \"./data/bio_group1_train.conllu\"\n",
    "VALID_FILE = \"./data/bio_group1_test.conllu\"\n",
    "#TRAIN_FILE = \"./data/xa_naverToCoNLL\"\n",
    "#VALID_FILE = \"./data/xa\"\n",
    "#TRAIN_FILE = \"/home/ktlim/code/TTtagger/corpus/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-train.conllu\"\n",
    "#VALID_FILE = \"/home/ktlim/code/TTtagger/corpus/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=0\n",
    "ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(word):\n",
    "    w = re.sub(r\".+@.+\", \"*EMAIL*\", word)\n",
    "    w = re.sub(r\"@\\w+\", \"*AT*\", w)\n",
    "    w = re.sub(r\"(https?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"(http?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"(HTTPS?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"(HTTP?://|www\\.).*\", \"*url*\", w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def strong_normalize(word):\n",
    "    w = ftfy.fix_text(word.lower())\n",
    "    w = re.sub(r\".+@.+\", \"*EMAIL*\", w)\n",
    "    w = re.sub(r\"@\\w+\", \"*AT*\", w)\n",
    "    w = re.sub(r\"(https?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"(http?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"([^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"([^\\d][^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"``\", '\"', w)\n",
    "    w = re.sub(r\"''\", '\"', w)\n",
    "    w = re.sub(r\"\\d\", \"0\", w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def buildVocab(graphs, cutoff=1):\n",
    "    wordsCount = Counter()\n",
    "    charsCount = Counter()\n",
    "    uposCount = Counter()\n",
    "    xposCount = Counter()\n",
    "    relCount = Counter()\n",
    "    featCount = Counter()\n",
    "    langCount = Counter()\n",
    "\n",
    "    for graph in graphs:\n",
    "        wordsCount.update([node.norm for node in graph.nodes[1:]])\n",
    "        for node in graph.nodes[1:]:\n",
    "            charsCount.update(list(node.word))\n",
    "            featCount.update(node.feats_set)\n",
    "            #  charsCount.update(list(node.norm))\n",
    "        uposCount.update([node.upos for node in graph.nodes[1:]])\n",
    "        xposCount.update([node.xupos for node in graph.nodes[1:]])\n",
    "        relCount.update([rel for rel in graph.rels[1:]])\n",
    "        langCount.update([node.lang for node in graph.nodes[1:]])\n",
    "        \n",
    "\n",
    "    wordsCount = Counter({w: i for w, i in wordsCount.items() if i >= cutoff})\n",
    "    print(\"Vocab containing {} words\".format(len(wordsCount)))\n",
    "    print(\"Charset containing {} chars\".format(len(charsCount)))\n",
    "    print(\"UPOS containing {} tags\".format(len(uposCount)), uposCount)\n",
    "    print(\"XPOS containing {} tags\".format(len(xposCount)), xposCount)\n",
    "    print(\"Rels containing {} tags\".format(len(relCount)), relCount)\n",
    "    print(\"Feats containing {} tags\".format(len(featCount)), featCount)\n",
    "    print(\"lang containing {} tags\".format(len(langCount)), langCount)\n",
    "\n",
    "    ret = {\n",
    "        \"vocab\": list(wordsCount.keys()),\n",
    "        \"wordfreq\": wordsCount,\n",
    "        \"charset\": list(charsCount.keys()),\n",
    "        \"charfreq\": charsCount,\n",
    "        \"upos\": list(uposCount.keys()),\n",
    "        \"xpos\": list(xposCount.keys()),\n",
    "        \"rels\": list(relCount.keys()),\n",
    "        \"feats\": list(featCount.keys()),\n",
    "        \"lang\": list(langCount.keys()),\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def shuffled_stream(data):\n",
    "    len_data = len(data)\n",
    "    while True:\n",
    "        for d in random.sample(data, len_data):\n",
    "            yield d\n",
    "\n",
    "def shuffled_balanced_stream(data):\n",
    "    for ds in zip(*[shuffled_stream(s) for s in data]):\n",
    "        ds = list(ds)\n",
    "        random.shuffle(ds)\n",
    "        for d in ds:\n",
    "            yield d\n",
    "            \n",
    "            \n",
    "def parse_dict(features):\n",
    "    if features is None or features == \"_\":\n",
    "        return {}\n",
    "\n",
    "    ret = {}\n",
    "    lst = features.split(\"|\")\n",
    "    for l in lst:\n",
    "        k, v = l.split(\"=\")\n",
    "        ret[k] = v\n",
    "    return ret\n",
    "\n",
    "\n",
    "def parse_features(features):\n",
    "    if features is None or features == \"_\":\n",
    "        return set()\n",
    "\n",
    "    return features.lower().split(\"|\")\n",
    "\n",
    "\n",
    "class Word:\n",
    "\n",
    "    def __init__(self, word, upos, lemma=None, xpos=None, feats=None, misc=None, lang=None):\n",
    "        self.word = normalize(word)\n",
    "        self.norm = strong_normalize(word) #strong_normalize(word)\n",
    "        self.lemma = lemma if lemma else \"_\"\n",
    "        self.upos = upos\n",
    "        self.xpos = xpos if xpos else \"_\"\n",
    "        self.xupos = self.upos + \"|\" + self.xpos\n",
    "        self.feats = feats if feats else \"_\"\n",
    "        self.feats_set = parse_features(self.feats)\n",
    "        self.misc = misc if misc else \"_\"\n",
    "        self.lang = lang if lang else \"_\"\n",
    "\n",
    "    def cleaned(self):\n",
    "        return Word(self.word, \"_\")\n",
    "\n",
    "    def clone(self):\n",
    "        return Word(self.word, self.upos, self.lemma, self.xpos, self.feats, self.misc)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}_{}\".format(self.word, self.upos)\n",
    "\n",
    "\n",
    "class DependencyGraph(object):\n",
    "\n",
    "    def __init__(self, words, tokens=None):\n",
    "        #  Token is a tuple (start, end, form)\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        self.nodes = np.array([Word(\"*root*\", \"*root*\")] + list(words))\n",
    "        self.tokens = tokens\n",
    "        self.heads = np.array([-1] * len(self.nodes))\n",
    "        self.rels = np.array([\"_\"] * len(self.nodes), dtype=object)\n",
    "\n",
    "    def __copy__(self):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        result.nodes = self.nodes\n",
    "        result.tokens = self.tokens\n",
    "        result.heads = self.heads.copy()\n",
    "        result.rels = self.rels.copy()\n",
    "        return result\n",
    "\n",
    "    def cleaned(self, node_level=True):\n",
    "        if node_level:\n",
    "            return DependencyGraph([node.cleaned() for node in self.nodes[1:]], self.tokens)\n",
    "        else:\n",
    "            return DependencyGraph([node.clone() for node in self.nodes[1:]], self.tokens)\n",
    "\n",
    "    def attach(self, head, tail, rel):\n",
    "        self.heads[tail] = head\n",
    "        self.rels[tail] = rel\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([\"{} ->({})  {} ({})\".format(str(self.nodes[i]), self.rels[i], self.heads[i], self.nodes[self.heads[i]]) for i in range(len(self.nodes))])\n",
    "\n",
    "\n",
    "def read_conll(filename, lang_code=None, max_token=80):\n",
    "    \n",
    "    print(\"read_conll with\", lang_code)\n",
    "    def get_word(columns):\n",
    "        return Word(columns[FORM], columns[UPOS], lemma=columns[LEMMA], xpos=columns[XPOS], feats=columns[FEATS], misc=columns[MISC], lang=lang_code)\n",
    "\n",
    "    def get_graph(graphs, words, tokens, edges):\n",
    "        graph = DependencyGraph(words, tokens)\n",
    "        for (h, d, r) in edges:\n",
    "            graph.attach(h, d, r)\n",
    "        graphs.append(graph)\n",
    "\n",
    "    file = open(filename, \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "    graphs = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    edges = []\n",
    "\n",
    "    num_sent = 0\n",
    "    sentence_start = False\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            if len(words) > 0:\n",
    "                get_graph(graphs, words, tokens, edges)\n",
    "                words, tokens, edges = [], [], []\n",
    "            break\n",
    "        line = line.rstrip(\"\\r\\n\")\n",
    "\n",
    "        # Handle sentence start boundaries\n",
    "        if not sentence_start:\n",
    "            # Skip comments\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            # Start a new sentence\n",
    "            sentence_start = True\n",
    "        if not line:\n",
    "            sentence_start = False\n",
    "            if len(words) > 0:\n",
    "                if (len(words) < max_token):\n",
    "                    get_graph(graphs, words, tokens, edges)\n",
    "                words, tokens, edges = [], [], []\n",
    "                num_sent += 1\n",
    "            continue\n",
    "\n",
    "        # Read next token/word\n",
    "        columns = line.split(\"\\t\")\n",
    "\n",
    "        # Skip empty nodes\n",
    "        if \".\" in columns[ID]:\n",
    "            continue\n",
    "\n",
    "        # Handle multi-word tokens to save word(s)\n",
    "        if \"-\" in columns[ID]:\n",
    "            start, end = map(int, columns[ID].split(\"-\"))\n",
    "            tokens.append((start, end + 1, columns[FORM]))\n",
    "\n",
    "            for _ in range(start, end + 1):\n",
    "                word_line = file.readline().rstrip(\"\\r\\n\")\n",
    "                word_columns = word_line.split(\"\\t\")\n",
    "                #LKT added for making a limitation for the word length to 30!!!\n",
    "                #if len(get_word(word_columns).word) >30:\n",
    "                    #print(get_word(word_columns))\n",
    "                words.append(get_word(word_columns))\n",
    "                if word_columns[HEAD].isdigit():\n",
    "                    head = int(word_columns[HEAD])\n",
    "                else:\n",
    "                    head = -1\n",
    "                edges.append((head, int(word_columns[ID]), word_columns[DEPREL].split(\":\")[0]))\n",
    "        # Basic tokens/words\n",
    "        else:\n",
    "            #LKT added for making a limitation for the word length to 30!!!\n",
    "            #if len(get_word(columns).word) > 30:\n",
    "                #print(get_word(columns))\n",
    "            words.append(get_word(columns))\n",
    "            if columns[HEAD].isdigit():\n",
    "                head = int(columns[HEAD])\n",
    "            else:\n",
    "                head = -1\n",
    "            edges.append((head, int(columns[ID]), columns[DEPREL].split(\":\")[0]))\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def naver_to_conll(read_file, write_file):\n",
    "    with open(WRITE_FILE,\"w\", encoding=\"UTF-8\") as write_file:\n",
    "        with open(READ_FILE, \"r\", encoding=\"UTF-8\") as read_file:\n",
    "            for line in read_file.readlines():\n",
    "                if line.startswith(\"# sent_id = \"):\n",
    "                    write_file.writelines('\\n')\n",
    "                    write_file.writelines(line)\n",
    "                elif line.startswith(\"# ner = \") or line.startswith(\"# text = \"):\n",
    "                    write_file.writelines(line)\n",
    "\n",
    "                if line.startswith(\"# ner = \"):\n",
    "                    trim_line = line.replace(\"# ner = \", \"\").replace('\\n', \"\")\n",
    "                    tokens = trim_line.split(' ')\n",
    "                    token_idx = 1\n",
    "                    for token in tokens:\n",
    "                        out = token.split('/')\n",
    "                        word, tag = out[0], out[1]\n",
    "                        a_line = str(token_idx)+'\\t'+str(word)+'\\t'+str(word)+'\\t'+str('NOUN')+'\\t'+str('NNG')+'\\t'+str(tag).replace('-','O')+'\\t'+str('_')+'\\t'+str('_')+'\\t'+str('_')+'\\t'+str('_')+'\\n'\n",
    "                        token_idx = token_idx+1 \n",
    "                        write_file.writelines(a_line)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loader\n",
    "class CoNLLDataset:\n",
    "    def __init__(self, graphs, tokenizer, max_len, fullvocab=None):\n",
    "        self.conll_graphs = graphs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self._fullvocab = fullvocab if fullvocab else buildVocab(self.conll_graphs, cutoff=1)\n",
    "            \n",
    "        self._upos = {p: i for i, p in enumerate(self._fullvocab[\"upos\"])}\n",
    "        self._iupos = self._fullvocab[\"upos\"]\n",
    "        self._xpos = {p: i for i, p in enumerate(self._fullvocab[\"xpos\"])}\n",
    "        self._ixpos = self._fullvocab[\"xpos\"]\n",
    "        self._vocab = {w: i+3 for i, w in enumerate(self._fullvocab[\"vocab\"])}\n",
    "        self._wordfreq = self._fullvocab[\"wordfreq\"]\n",
    "        self._charset = {c: i+3 for i, c in enumerate(self._fullvocab[\"charset\"])}\n",
    "        self._charfreq = self._fullvocab[\"charfreq\"]\n",
    "        self._rels = {r: i for i, r in enumerate(self._fullvocab[\"rels\"])}\n",
    "        self._irels = self._fullvocab[\"rels\"]\n",
    "        self._feats = {f: i for i, f in enumerate(self._fullvocab[\"feats\"])}\n",
    "        self._ifeats = self._fullvocab[\"feats\"]\n",
    "        self._langs = {r: i+2 for i, r in enumerate(self._fullvocab[\"lang\"])}\n",
    "        self._ilangs = self._fullvocab[\"lang\"]\n",
    "        \n",
    "        #self._posRels = {r: i for i, r in enumerate(self._fullvocab[\"posRel\"])}\n",
    "        #self._iposRels = self._fullvocab[\"posRel\"]\n",
    "        \n",
    "        self._vocab['*pad*'] = 0\n",
    "        self._charset['*pad*'] = 0\n",
    "        self._langs['*pad*'] = 0\n",
    "        \n",
    "        self._vocab['*root*'] = 1\n",
    "        self._charset['*whitespace*'] = 1\n",
    "        \n",
    "        self._vocab['*unknown*'] = 2\n",
    "        self._charset['*unknown*'] = 2\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.conll_graphs)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        graph = self.conll_graphs[item]\n",
    "        word_list = [node.word for node in graph.nodes]\n",
    "        upos_list = [node.xpos for node in graph.nodes]\n",
    "        feat_list = [node.feats for node in graph.nodes]\n",
    "        \n",
    "        encoded = self.tokenizer.encode_plus(' '.join(word_list[1:]),\n",
    "                                             None,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length = self.max_len,\n",
    "                                             truncation=True,\n",
    "                                             pad_to_max_length = True)\n",
    "        \n",
    "        ids, mask = encoded['input_ids'], encoded['attention_mask']\n",
    "        \n",
    "        bpe_head_mask = [0]; upos_ids = [0]; feat_ids = [-1] # --> CLS token\n",
    "        \n",
    "        for word, upos, feat in zip(word_list[1:], upos_list[1:], feat_list[1:]):\n",
    "            bpe_len = len(self.tokenizer.tokenize(word))\n",
    "            head_mask = [1] + [0]*(bpe_len-1)\n",
    "            bpe_head_mask.extend(head_mask)\n",
    "            upos_mask = [self._upos.get(upos, 0)] + [0]*(bpe_len-1)\n",
    "            upos_ids.extend(upos_mask)\n",
    "            feat_mask = [self._feats.get(feat.lower(), 2)] + [-1]*(bpe_len-1)\n",
    "            feat_ids.extend(feat_mask)\n",
    "            \n",
    "            #print(\"head_mask\", head_mask)\n",
    "        \n",
    "        bpe_head_mask.append(0); upos_ids.append(0); feat_ids.append(-1) # --> END token\n",
    "        bpe_head_mask.extend([0] * (self.max_len - len(bpe_head_mask))) ## --> padding by max_len\n",
    "        upos_ids.extend([0] * (self.max_len - len(upos_ids))) ## --> padding by max_len\n",
    "        feat_ids.extend([-1] * (self.max_len - len(feat_ids))) ## --> padding by max_len\n",
    "        \n",
    "        return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'bpe_head_mask': torch.tensor(bpe_head_mask, dtype=torch.long),\n",
    "                'upos_ids': torch.tensor(upos_ids, dtype=torch.long),\n",
    "                'feat_ids': torch.tensor(feat_ids, dtype=torch.long)\n",
    "               }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(total_pred, total_targ, noNER_idx):\n",
    "    \n",
    "    p = 0 # (retrived SB and real SB) / retrived SB  # The percentage of (the number of correct predictions) / (the number of predction that system predicts as B-SENT)\n",
    "    r = 0\n",
    "    f1= 0\n",
    "    \n",
    "    np_total_pred = np.array(total_pred)\n",
    "    np_total_tag = np.array(total_targ)\n",
    "    \n",
    "    #Get noPad\n",
    "    incidence_nopad = np.where(np_total_tag != -1) ## eliminate paddings\n",
    "    np_total_pred_nopad = np_total_pred[incidence_nopad]\n",
    "    np_total_tag_nopad = np_total_tag[incidence_nopad]\n",
    "    \n",
    "    \n",
    "    #precision\n",
    "    incidence_nopad_sb = np.where(np_total_pred_nopad != noNER_idx)\n",
    "    np_total_pred_nopad_sb = np_total_pred_nopad[incidence_nopad_sb]\n",
    "    np_total_tag_nopad_sb = np_total_tag_nopad[incidence_nopad_sb]\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_nopad_sb)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_nopad_sb==np_total_tag_nopad_sb) == True)\n",
    "    \n",
    "    '''\n",
    "    np_total_pred_incid = np_total_pred[incidence_p]\n",
    "    print(\"np_total_pred_incid\", np_total_pred_incid)\n",
    "    ids_sb_pred_p = np.where(np_total_pred_incid==1)\n",
    "    np_total_pred_p = np_total_pred_incid[ids_sb_pred_p]\n",
    "    np_total_tag_p = np_total_tag[ids_sb_pred_p]\n",
    "    \n",
    "    print(\"ids_sb_pred_p\", ids_sb_pred_p)\n",
    "    print(\"np_total_pred_p\", np_total_pred_p)\n",
    "    print(\"np_total_tag_p\", np_total_tag_p)\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_p)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_p==np_total_tag_p) == True)\n",
    "    '''\n",
    "    \n",
    "    print(\"count_correct_p\", count_correct_p)\n",
    "    print(\"count_active_tokens_p\", count_active_tokens_p)\n",
    "    \n",
    "    p = count_correct_p/count_active_tokens_p\n",
    "    print(\"precision:\", p)\n",
    "\n",
    "    \n",
    "    #recall\n",
    "    ids_sb_pred_r = np.where(np_total_tag_nopad != noNER_idx)\n",
    "    np_total_pred_r = np_total_pred_nopad[ids_sb_pred_r]\n",
    "    np_total_tag_r = np_total_tag_nopad[ids_sb_pred_r]\n",
    "    \n",
    "    #print(\"ids_sb_pred_r\", ids_sb_pred_r)\n",
    "    #print(\"np_total_pred_r\", np_total_pred_r)\n",
    "    #print(\"np_total_tag_r\", np_total_tag_r)\n",
    "    \n",
    "    count_active_tokens_r = len(np_total_pred_r)\n",
    "    count_correct_r = np.count_nonzero((np_total_pred_r==np_total_tag_r) == True)\n",
    "    \n",
    "    print(\"count_active_tokens_r\", count_active_tokens_r)\n",
    "    print(\"count_correct_r\", count_correct_r)\n",
    "    \n",
    "    r = count_correct_r/count_active_tokens_r\n",
    "    print(\"recall:\", r)\n",
    "    \n",
    "    \n",
    "    #F1\n",
    "    #f1 = 2*(p*r) / (p+r)\n",
    "    print(\"F1:\", f1)\n",
    "    \n",
    "    \n",
    "    return p, r\n",
    "    \n",
    "    #count_active_tokens_recall = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    #print(\"count_active_tokens_recall\", count_active_tokens_recall)\n",
    "    #count_active_tokens_precision = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    \n",
    "    #count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"count_correct\",count_correct)\n",
    "    #print(\"ACCURACY:\", count_correct/count_active_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaEncoder(nn.Module):\n",
    "    def __init__(self, num_upos, num_feat):\n",
    "        super(XLMRobertaEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = 768\n",
    "        self.xlm_roberta = transformers.BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "        self.pos_dim = 50\n",
    "        self.pos_emb = nn.Embedding(num_upos, self.pos_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear = nn.Linear(self.hidden_size+self.pos_dim, num_upos)\n",
    "        \n",
    "        self.f_dropout = nn.Dropout(0.33)\n",
    "        self.f_linear = nn.Linear(768+self.pos_dim, num_feat)\n",
    "        \n",
    "        # Take the CRF module at https://github.com/eagle705/pytorch-bert-crf-ner\n",
    "        self.crf = CRF(num_tags=num_feat, batch_first=True)\n",
    "        \n",
    "            \n",
    "    def forward(self, ids, mask, pos_ids, targets=None):\n",
    "        o1, o2 = self.xlm_roberta(ids, mask)\n",
    "        \n",
    "        #apool = torch.mean(o1, 1)\n",
    "        #mpool, _ = torch.max(o1, 1)\n",
    "        #cat = torch.cat((apool, mpool), 1)\n",
    "        #bo = self.dropout(cat)\n",
    "\n",
    "        #print(\"pos_ids\", pos_ids, pos_ids.size())\n",
    "        pos_vec = self.pos_emb(pos_ids)\n",
    "\n",
    "        #print(\"dims\", pos_vec.size(), o1.size())\n",
    "        o1 = torch.cat([pos_vec, o1], dim=2)\n",
    "        #print(\"after_dim:\", o1.size())\n",
    "        o1 = self.f_dropout(o1)\n",
    "        p_logits = None #self.linear(o1)        \n",
    "        f_logits = self.f_linear(o1)   \n",
    "        \n",
    "        \n",
    "        if targets is not None:\n",
    "            log_likelihood, crf_out = self.crf(f_logits, targets), self.crf.decode(f_logits)\n",
    "            return p_logits, f_logits, log_likelihood, crf_out #since the log_likelihood is a number, it is not working on multi-gpu, probably it should be a tensor\n",
    "        else:\n",
    "            crf_out = self.crf.decode(f_logits)\n",
    "            return p_logits, f_logits, crf_out\n",
    "        \n",
    "        #print(crf_out, crf_out.size)\n",
    "        \n",
    "        \n",
    "        #if tags is not None: # crf training\n",
    "        #    log_likelihood, sequence_of_tags = self.crf(emissions, tags), self.crf.decode(emissions)\n",
    "        #    return log_likelihood, sequence_of_tags\n",
    "        #else: # tag inference\n",
    "        #    sequence_of_tags = self.crf.decode(emissions)\n",
    "        #    return sequence_of_tags, outputs\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"xpos_bert_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_conll with ko\n",
      "Vocab containing 4528 words\n",
      "Charset containing 1027 chars\n",
      "UPOS containing 15 tags Counter({'NOUN': 5257, 'PRT': 3801, 'ADP': 2810, 'VERB': 1691, 'PUNCT': 1550, 'PROPN': 645, 'NUM': 557, 'X': 281, 'DET': 220, 'ADJ': 206, 'PRON': 117, 'SYM': 105, 'ADV': 71, 'INTJ': 8, 'CCONJ': 2})\n",
      "XPOS containing 42 tags Counter({'NOUN|NNG': 4545, 'VERB|VV': 1143, 'PRT|EC': 964, 'PRT|ETM': 904, 'ADP|JKB': 760, 'PUNCT|SF': 662, 'ADP|JKO': 655, 'PROPN|NNP': 645, 'NOUN|NNB': 623, 'PUNCT|SS': 617, 'ADP|JX': 613, 'NUM|SN': 557, 'PRT|EF': 554, 'PRT|XSV': 503, 'PRT|EP': 383, 'ADP|JKS': 380, 'ADP|JKG': 335, 'VERB|VCP': 276, 'X|SL': 275, 'PUNCT|SP': 271, 'PRT|XSN': 266, 'VERB|VX': 258, 'DET|MM': 220, 'ADJ|VA': 206, 'PRON|PRON': 117, 'SYM|SW': 96, 'PRT|XSA': 90, 'PRT|ETN': 88, 'ADV|MAG': 71, 'PRT|XPN': 49, 'NOUN|XR': 48, 'NOUN|NR': 41, 'ADP|JKC': 28, 'ADP|JKQ': 26, 'VERB|VCN': 14, 'ADP|JC': 13, 'INTJ|IC': 8, 'SYM|SO': 5, 'X|NA': 5, 'SYM|SE': 4, 'CCONJ|MAJ': 2, 'X|SH': 1})\n",
      "Rels containing 2 tags Counter({'rel': 16630, 'root': 691})\n",
      "Feats containing 49 tags Counter({'o': 13571, 'cvl_s': 385, 'num_b': 330, 'num_e': 330, 'per_s': 239, 'org_s': 172, 'num_i': 161, 'org_i': 151, 'dat_b': 130, 'dat_e': 129, 'loc_s': 121, 'org_e': 116, 'org_b': 114, 'dat_i': 106, 'trm_i': 104, 'cvl_b': 88, 'cvl_e': 88, 'trm_s': 80, 'trm_e': 79, 'trm_b': 78, 'per_b': 78, 'per_e': 77, 'num_s': 62, 'per_i': 61, 'cvl_i': 52, 'dat_s': 50, 'anm_s': 43, 'evt_b': 40, 'evt_e': 40, 'evt_s': 27, 'evt_i': 26, 'loc_b': 20, 'afw_s': 20, 'loc_e': 19, 'tim_b': 19, 'tim_e': 19, 'tim_i': 15, 'afw_i': 14, 'loc_i': 12, 'fld_s': 11, 'afw_b': 9, 'afw_e': 9, 'tim_s': 7, 'plt_s': 6, 'fld_b': 5, 'fld_e': 5, 'mat_s': 1, 'anm_b': 1, 'anm_e': 1})\n",
      "lang containing 1 tags Counter({'ko': 17321})\n",
      "read_conll with ko\n"
     ]
    }
   ],
   "source": [
    "train_graphs = read_conll(TRAIN_FILE, 'ko', MAX_TOKEN)\n",
    "train_dataset = CoNLLDataset(graphs=train_graphs, tokenizer=TOKENIZER, max_len=MAX_LEN)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=4, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "valid_graphs = read_conll(VALID_FILE, 'ko', MAX_TOKEN)\n",
    "valid_dataset = CoNLLDataset(graphs=valid_graphs, tokenizer=TOKENIZER, max_len=MAX_LEN, fullvocab=train_dataset._fullvocab)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=4, batch_size=VALID_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_upos = len(train_dataset._upos)\n",
    "num_feat = len(train_dataset._feats)\n",
    "model = XLMRobertaEncoder(num_upos, num_feat)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "lr = 0.000005\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(train_loader, model, optimizer, DEVICE, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    p_total_pred = []\n",
    "    p_total_targ = []\n",
    "    p_total_loss = []\n",
    "    \n",
    "    f_total_pred = []\n",
    "    f_total_targ = []\n",
    "    f_total_loss = []\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        p_logits, f_logits, likelihood, crf_out = model(batch['ids'].cuda(), batch['mask'].cuda(), batch['upos_ids'].cuda(), batch['feat_ids'].cuda())\n",
    "        \n",
    "        #UPOS\n",
    "        #b,s,l = p_logits.size()\n",
    "        #print(p_logits.view(b*s,l), p_logits.view(b*s,l).size())\n",
    "        #print(batch['upos_ids'].cuda().view(b*s), batch['upos_ids'].cuda().view(b*s).size())\n",
    "        \n",
    "        '''\n",
    "        p_loss = loss_fn(p_logits.view(b*s,l), batch['upos_ids'].cuda().view(b*s))\n",
    "        p_total_loss.append(p_loss.item())\n",
    "        p_total_pred.extend(torch.argmax(p_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        p_total_targ.extend(batch['upos_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        #FEAT\n",
    "        b,s,l = f_logits.size()\n",
    "        f_loss = loss_fn(f_logits.view(b*s,l), batch['feat_ids'].cuda().view(b*s))\n",
    "        f_total_loss.append(f_loss.item())\n",
    "        #f_total_pred.extend(torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        \n",
    "        #f_total_pred.extend(crf_out)\n",
    "        f_total_targ.extend(batch['feat_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        \n",
    "        for s in crf_out:\n",
    "            f_total_pred.extend(s)\n",
    "        \n",
    "        #loss = p_loss + (-1 * likelihood)\n",
    "        loss = -1 * likelihood\n",
    "        #print(\"crf_out\", crf_out)\n",
    "        #print(\"pred\", torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #count_active_tokens = np.count_nonzero(np.array(p_total_targ) > -1)\n",
    "    #count_correct = np.count_nonzero((np.array(p_total_pred)==np.array(p_total_targ)) == True)\n",
    "    #print(\"TRAINING POS ACCURACY:\", count_correct/count_active_tokens)\n",
    "    \n",
    "    count_active_tokens = np.count_nonzero(np.array(f_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(f_total_pred)==np.array(f_total_targ)) == True)\n",
    "    p, r = f1_score(f_total_pred, f_total_targ, train_dataset._feats.get('o', 2))\n",
    "    print(\"TRAINING FEAT ACCURACY:\", count_correct/count_active_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loop_fn(dev_loader, model, DEVICE):\n",
    "    model.eval()\n",
    "    \n",
    "    p_total_pred = []\n",
    "    p_total_targ = []\n",
    "    p_total_loss = []\n",
    "    \n",
    "    f_total_pred = []\n",
    "    f_total_targ = []\n",
    "    f_total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(dev_loader), total=len(dev_loader)):\n",
    "\n",
    "            p_logits, f_logits, likelihood, crf_out = model(batch['ids'].cuda(), batch['mask'].cuda(), batch['upos_ids'].cuda(), batch['feat_ids'].cuda())\n",
    "\n",
    "            #UPOS\n",
    "            #b,s,l = p_logits.size()\n",
    "            #print(p_logits.view(b*s,l), p_logits.view(b*s,l).size())\n",
    "            #print(batch['upos_ids'].cuda().view(b*s), batch['upos_ids'].cuda().view(b*s).size())\n",
    "            #p_loss = loss_fn(p_logits.view(b*s,l), batch['upos_ids'].cuda().view(b*s))\n",
    "            #p_total_loss.append(p_loss.item())\n",
    "            #p_total_pred.extend(torch.argmax(p_logits.view(b*s,l), 1).cpu().tolist())\n",
    "            #p_total_targ.extend(batch['upos_ids'].cuda().view(b*s).cpu().tolist())\n",
    "\n",
    "            #FEAT\n",
    "            b,s,l = f_logits.size()\n",
    "            f_loss = loss_fn(f_logits.view(b*s,l), batch['feat_ids'].cuda().view(b*s))\n",
    "            f_total_loss.append(f_loss.item())\n",
    "            #f_total_pred.extend(torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "\n",
    "            #f_total_pred.extend(crf_out)\n",
    "            f_total_targ.extend(batch['feat_ids'].cuda().view(b*s).cpu().tolist())\n",
    "\n",
    "            for s in crf_out:\n",
    "                f_total_pred.extend(s)\n",
    "\n",
    "            loss = f_loss\n",
    "        \n",
    "        \n",
    "    #count_active_tokens = np.count_nonzero(np.array(p_total_targ) > -1)\n",
    "    #count_correct = np.count_nonzero((np.array(p_total_pred)==np.array(p_total_targ)) == True)\n",
    "    #print(\"VALIDATION POS ACCURACY:\", count_correct/count_active_tokens)\n",
    "    \n",
    "    count_active_tokens = np.count_nonzero(np.array(f_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(f_total_pred)==np.array(f_total_targ)) == True)\n",
    "    p, r = f1_score(f_total_pred, f_total_targ, train_dataset._feats.get('o', 2))\n",
    "    print(\"VALIDATION FEAT ACCURACY:\", count_correct/count_active_tokens)\n",
    "    \n",
    "    return count_correct/count_active_tokens, p, r, f_total_pred, f_total_targ\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 1/39 [00:01<01:14,  1.97s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 15.75 GiB total capacity; 14.44 GiB already allocated; 11.56 MiB free; 14.69 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6c1e4d072783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d12751100383>\u001b[0m in \u001b[0;36mtrain_loop_fn\u001b[0;34m(train_loader, model, optimizer, DEVICE, scheduler)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mp_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'upos_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feat_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#UPOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f860536732d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, pos_ids, targets)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mo1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlm_roberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#apool = torch.mean(o1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 153\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \"\"\"\n\u001b[1;32m   1955\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 1956\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   1957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 15.75 GiB total capacity; 14.44 GiB already allocated; 11.56 MiB free; 14.69 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "LOAD_MODEL=False\n",
    "step=0\n",
    "best_score=0\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "    \n",
    "for idx in range(EPOCHS):\n",
    "    train_loop_fn(train_loader, model, optimizer, DEVICE)\n",
    "    acc, p, r, pred, targ = valid_loop_fn(valid_loader, model, DEVICE)\n",
    "    \n",
    "    if best_score <= acc:\n",
    "        best_score = acc\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"step\": step,\n",
    "            \"train_dataset\": train_dataset\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "        \n",
    "        filename= str(p)+\"_\"+str(r)\n",
    "        pred_np = np.array(pred)\n",
    "        targ_np = np.array(targ)\n",
    "        ids_pad = np.where(targ_np != -1)\n",
    "        pred_np_noPad = pred_np[ids_pad].tolist()\n",
    "        targ_np_noPad = targ_np[ids_pad].tolist()\n",
    "        \n",
    "        #print(pred_np_noPad)\n",
    "\n",
    "        with open(\"./results/xpos_bert_predicted_\"+filename, \"w\", encoding=\"UTF-8\") as write_file:            \n",
    "            pred_list = [ train_dataset._ifeats[p] for p in pred_np_noPad]\n",
    "            write_file.writelines(\"\\n\".join(pred_list))\n",
    "\n",
    "        with open(\"./results/xpos_bert_gold_\"+filename, \"w\", encoding=\"UTF-8\") as write_file:\n",
    "            gold_list = [ train_dataset._ifeats[p] for p in targ_np_noPad]\n",
    "            write_file.writelines(\"\\n\".join(gold_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"step\": step,\n",
    "    \"train_dataset\": train_dataset\n",
    "}\n",
    "save_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset._ifeats[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.4 on Python 3.6 (CUDA 10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
