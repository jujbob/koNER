{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "TRAIN_BATCH_SIZE = 40\n",
    "VALID_BATCH_SIZE = 40\n",
    "EPOCHS = 100\n",
    "BERT_MODEL = 'xlm-roberta-base'\n",
    "#BERT_MODEL = 'klue/roberta-base'\n",
    "TOKENIZER = transformers.AutoTokenizer.from_pretrained(BERT_MODEL, use_fast=True)\n",
    "\n",
    "DATASET = \"ETRI_syl\" #MODU21 #ETRI # MODU19_syl #MODU21_syl #ETRI_syl\n",
    "VALID_FILE = None\n",
    "if DATASET == \"NAVER\":\n",
    "    #TRAIN_FILE = \"./data/bio_group1_test.conllu\"\n",
    "    #VALID_FILE = \"./data/bio_group1_test.conllu\"\n",
    "    TRAIN_FILE = \"./data/train_data.bio.converted\"\n",
    "elif DATASET == \"KLUE\":\n",
    "    TRAIN_FILE = \"./data/klue-ner-v1.1_train.converted\"\n",
    "    VALID_FILE = \"./data/klue-ner-v1.1_dev.converted\"\n",
    "elif DATASET == \"MODU19\":\n",
    "    TRAIN_FILE = \"./data/NXNE2102008030.converted\" #MODU19\n",
    "elif DATASET == \"MODU21\":\n",
    "    TRAIN_FILE = \"./data/NXNE2102203310.converted\" #MODU21\n",
    "elif DATASET == \"ETRI\":\n",
    "    TRAIN_FILE = \"./data/EXOBRAIN_NE_CORPUS_10000.converted\" #ETRI15\n",
    "elif DATASET == \"MODU19_syl\":\n",
    "    TRAIN_FILE = \"./data/NXNE2102008030.corrected\" #MODU19\n",
    "elif DATASET == \"MODU21_syl\":\n",
    "    TRAIN_FILE = \"./data/NXNE2102203310.corrected\" #MODU21\n",
    "elif DATASET == \"ETRI_syl\":\n",
    "    TRAIN_FILE = \"./data/EXOBRAIN_NE_CORPUS_10000.corrected_conll\" #ETRI15\n",
    "else:\n",
    "    print(\"Please set your DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=0\n",
    "ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(word):\n",
    "    return re.sub(r\"\\d\", \"0\", word).lower()\n",
    "\n",
    "\n",
    "def strong_normalize(word):\n",
    "    w = ftfy.fix_text(word.lower())\n",
    "    w = re.sub(r\".+@.+\", \"*EMAIL*\", w)\n",
    "    w = re.sub(r\"@\\w+\", \"*AT*\", w)\n",
    "    w = re.sub(r\"(https?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"([^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"([^\\d][^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"``\", '\"', w)\n",
    "    w = re.sub(r\"''\", '\"', w)\n",
    "    w = re.sub(r\"\\d\", \"0\", w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def buildVocab(graphs, cutoff=1):\n",
    "    wordsCount = Counter()\n",
    "    charsCount = Counter()\n",
    "    uposCount = Counter()\n",
    "    xposCount = Counter()\n",
    "    relCount = Counter()\n",
    "    featCount = Counter()\n",
    "    langCount = Counter()\n",
    "\n",
    "    for graph in graphs:\n",
    "        wordsCount.update([node.norm for node in graph.nodes[1:]])\n",
    "        for node in graph.nodes[1:]:\n",
    "            charsCount.update(list(node.word))\n",
    "            featCount.update(node.feats_set)\n",
    "            #  charsCount.update(list(node.norm))\n",
    "        uposCount.update([node.upos for node in graph.nodes[1:]])\n",
    "        xposCount.update([node.xupos for node in graph.nodes[1:]])\n",
    "        relCount.update([rel for rel in graph.rels[1:]])\n",
    "        langCount.update([node.lang for node in graph.nodes[1:]])\n",
    "        \n",
    "\n",
    "    wordsCount = Counter({w: i for w, i in wordsCount.items() if i >= cutoff})\n",
    "    print(\"Vocab containing {} words\".format(len(wordsCount)))\n",
    "    print(\"Charset containing {} chars\".format(len(charsCount)))\n",
    "    print(\"UPOS containing {} tags\".format(len(uposCount)), uposCount)\n",
    "    #print(\"XPOS containing {} tags\".format(len(xposCount)), xposCount)\n",
    "    print(\"Rels containing {} tags\".format(len(relCount)), relCount)\n",
    "    print(\"Feats containing {} tags\".format(len(featCount)), featCount)\n",
    "    print(\"lang containing {} tags\".format(len(langCount)), langCount)\n",
    "\n",
    "    ret = {\n",
    "        \"vocab\": list(wordsCount.keys()),\n",
    "        \"wordfreq\": wordsCount,\n",
    "        \"charset\": list(charsCount.keys()),\n",
    "        \"charfreq\": charsCount,\n",
    "        \"upos\": list(uposCount.keys()),\n",
    "        \"xpos\": list(xposCount.keys()),\n",
    "        \"rels\": list(relCount.keys()),\n",
    "        \"feats\": list(featCount.keys()),\n",
    "        \"lang\": list(langCount.keys()),\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def shuffled_stream(data):\n",
    "    len_data = len(data)\n",
    "    while True:\n",
    "        for d in random.sample(data, len_data):\n",
    "            yield d\n",
    "\n",
    "def shuffled_balanced_stream(data):\n",
    "    for ds in zip(*[shuffled_stream(s) for s in data]):\n",
    "        ds = list(ds)\n",
    "        random.shuffle(ds)\n",
    "        for d in ds:\n",
    "            yield d\n",
    "            \n",
    "            \n",
    "def parse_dict(features):\n",
    "    if features is None or features == \"_\":\n",
    "        return {}\n",
    "\n",
    "    ret = {}\n",
    "    lst = features.split(\"|\")\n",
    "    for l in lst:\n",
    "        k, v = l.split(\"=\")\n",
    "        ret[k] = v\n",
    "    return ret\n",
    "\n",
    "\n",
    "def parse_features(features):\n",
    "    if features is None or features == \"_\":\n",
    "        return set()\n",
    "\n",
    "    return features.lower().split(\"|\")\n",
    "\n",
    "\n",
    "class Word:\n",
    "\n",
    "    def __init__(self, word, upos, lemma=None, xpos=None, feats=None, misc=None, lang=None):\n",
    "        self.word = word\n",
    "        self.norm = normalize(word) #strong_normalize(word)\n",
    "        self.lemma = lemma if lemma else \"_\"\n",
    "        self.upos = upos\n",
    "        self.xpos = xpos if xpos else \"_\"\n",
    "        self.xupos = self.upos + \"|\" + self.xpos\n",
    "        self.feats = feats if feats else \"_\"\n",
    "        self.feats_set = parse_features(self.feats)\n",
    "        self.misc = misc if misc else \"_\"\n",
    "        self.lang = lang if lang else \"_\"\n",
    "\n",
    "    def cleaned(self):\n",
    "        return Word(self.word, \"_\")\n",
    "\n",
    "    def clone(self):\n",
    "        return Word(self.word, self.upos, self.lemma, self.xpos, self.feats, self.misc)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}_{}\".format(self.word, self.upos)\n",
    "\n",
    "\n",
    "class DependencyGraph(object):\n",
    "\n",
    "    def __init__(self, words, tokens=None):\n",
    "        #  Token is a tuple (start, end, form)\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        self.nodes = np.array([Word(\"*root*\", \"*root*\")] + list(words))\n",
    "        self.tokens = tokens\n",
    "        self.heads = np.array([-1] * len(self.nodes))\n",
    "        self.rels = np.array([\"_\"] * len(self.nodes), dtype=object)\n",
    "\n",
    "    def __copy__(self):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        result.nodes = self.nodes\n",
    "        result.tokens = self.tokens\n",
    "        result.heads = self.heads.copy()\n",
    "        result.rels = self.rels.copy()\n",
    "        return result\n",
    "\n",
    "    def cleaned(self, node_level=True):\n",
    "        if node_level:\n",
    "            return DependencyGraph([node.cleaned() for node in self.nodes[1:]], self.tokens)\n",
    "        else:\n",
    "            return DependencyGraph([node.clone() for node in self.nodes[1:]], self.tokens)\n",
    "\n",
    "    def attach(self, head, tail, rel):\n",
    "        self.heads[tail] = head\n",
    "        self.rels[tail] = rel\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([\"{} ->({})  {} ({})\".format(str(self.nodes[i]), self.rels[i], self.heads[i], self.nodes[self.heads[i]]) for i in range(len(self.nodes))])\n",
    "\n",
    "\n",
    "def read_conll(filename, lang_code=None):\n",
    "    \n",
    "    print(\"read_conll with\", lang_code)\n",
    "    def get_word(columns):\n",
    "        return Word(columns[FORM], columns[UPOS], lemma=columns[LEMMA], xpos=columns[XPOS], feats=columns[FEATS], misc=columns[MISC], lang=lang_code)\n",
    "\n",
    "    def get_graph(graphs, words, tokens, edges):\n",
    "        graph = DependencyGraph(words, tokens)\n",
    "        for (h, d, r) in edges:\n",
    "            graph.attach(h, d, r)\n",
    "        graphs.append(graph)\n",
    "\n",
    "    file = open(filename, \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "    graphs = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    edges = []\n",
    "\n",
    "    num_sent = 0\n",
    "    sentence_start = False\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            if len(words) > 0:\n",
    "                get_graph(graphs, words, tokens, edges)\n",
    "                words, tokens, edges = [], [], []\n",
    "            break\n",
    "        line = line.rstrip(\"\\r\\n\")\n",
    "\n",
    "        # Handle sentence start boundaries\n",
    "        if not sentence_start:\n",
    "            # Skip comments\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            # Start a new sentence\n",
    "            sentence_start = True\n",
    "        if not line:\n",
    "            sentence_start = False\n",
    "            if len(words) > 0:\n",
    "                if (len(words) < 250):\n",
    "                    get_graph(graphs, words, tokens, edges)\n",
    "                words, tokens, edges = [], [], []\n",
    "                num_sent += 1\n",
    "            continue\n",
    "\n",
    "        # Read next token/word\n",
    "        columns = line.split(\"\\t\")\n",
    "\n",
    "        # Skip empty nodes\n",
    "        if \".\" in columns[ID]:\n",
    "            continue\n",
    "\n",
    "        # Handle multi-word tokens to save word(s)\n",
    "        if \"-\" in columns[ID]:\n",
    "            start, end = map(int, columns[ID].split(\"-\"))\n",
    "            tokens.append((start, end + 1, columns[FORM]))\n",
    "\n",
    "            for _ in range(start, end + 1):\n",
    "                word_line = file.readline().rstrip(\"\\r\\n\")\n",
    "                word_columns = word_line.split(\"\\t\")\n",
    "                words.append(get_word(word_columns))\n",
    "                if word_columns[HEAD].isdigit():\n",
    "                    head = int(word_columns[HEAD])\n",
    "                else:\n",
    "                    head = -1\n",
    "                edges.append((head, int(word_columns[ID]), word_columns[DEPREL].split(\":\")[0]))\n",
    "        # Basic tokens/words\n",
    "        else:\n",
    "            words.append(get_word(columns))\n",
    "            if columns[HEAD].isdigit():\n",
    "                head = int(columns[HEAD])\n",
    "            else:\n",
    "                head = -1\n",
    "            edges.append((head, int(columns[ID]), columns[DEPREL].split(\":\")[0]))\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loader\n",
    "class CoNLLDataset:\n",
    "    def __init__(self, graphs, tokenizer, max_len, fullvocab=None):\n",
    "        self.conll_graphs = graphs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self._fullvocab = fullvocab if fullvocab else buildVocab(self.conll_graphs, cutoff=1)\n",
    "            \n",
    "        self._upos = {p: i for i, p in enumerate(self._fullvocab[\"upos\"])}\n",
    "        self._iupos = self._fullvocab[\"upos\"]\n",
    "        self._xpos = {p: i for i, p in enumerate(self._fullvocab[\"xpos\"])}\n",
    "        self._ixpos = self._fullvocab[\"xpos\"]\n",
    "        self._vocab = {w: i+3 for i, w in enumerate(self._fullvocab[\"vocab\"])}\n",
    "        self._wordfreq = self._fullvocab[\"wordfreq\"]\n",
    "        self._charset = {c: i+3 for i, c in enumerate(self._fullvocab[\"charset\"])}\n",
    "        self._charfreq = self._fullvocab[\"charfreq\"]\n",
    "        self._rels = {r: i for i, r in enumerate(self._fullvocab[\"rels\"])}\n",
    "        self._irels = self._fullvocab[\"rels\"]\n",
    "        self._feats = {f: i for i, f in enumerate(self._fullvocab[\"feats\"])}\n",
    "        self._langs = {r: i+2 for i, r in enumerate(self._fullvocab[\"lang\"])}\n",
    "        self._ilangs = self._fullvocab[\"lang\"]\n",
    "        \n",
    "        #self._posRels = {r: i for i, r in enumerate(self._fullvocab[\"posRel\"])}\n",
    "        #self._iposRels = self._fullvocab[\"posRel\"]\n",
    "        \n",
    "        self._vocab['*pad*'] = 0\n",
    "        self._charset['*pad*'] = 0\n",
    "        self._langs['*pad*'] = 0\n",
    "        \n",
    "        self._vocab['*root*'] = 1\n",
    "        self._charset['*whitespace*'] = 1\n",
    "        \n",
    "        self._vocab['*unknown*'] = 2\n",
    "        self._charset['*unknown*'] = 2\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.conll_graphs)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        graph = self.conll_graphs[item]\n",
    "        word_list = [node.word for node in graph.nodes]\n",
    "        upos_list = [node.upos for node in graph.nodes]\n",
    "        feat_list = [node.feats for node in graph.nodes]\n",
    "        \n",
    "        encoded = self.tokenizer.encode_plus(' '.join(word_list[1:]),\n",
    "                                             None,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length = self.max_len,\n",
    "                                             truncation=True,\n",
    "                                             pad_to_max_length = True)\n",
    "        \n",
    "        ids, mask = encoded['input_ids'], encoded['attention_mask']\n",
    "        \n",
    "        bpe_head_mask = [0]; upos_ids = [-1]; feat_ids = [-1] # --> CLS token\n",
    "        \n",
    "        for word, upos, feat in zip(word_list[1:], upos_list[1:], feat_list[1:]):\n",
    "            bpe_len = len(self.tokenizer.tokenize(word))\n",
    "            head_mask = [1] + [0]*(bpe_len-1)\n",
    "            bpe_head_mask.extend(head_mask)\n",
    "            upos_mask = [self._upos.get(upos)] + [-1]*(bpe_len-1)\n",
    "            upos_ids.extend(upos_mask)\n",
    "            feat_mask = [self._feats.get(feat.lower(), 2)] + [-1]*(bpe_len-1)\n",
    "            feat_ids.extend(feat_mask)\n",
    "            \n",
    "            #print(\"head_mask\", head_mask)\n",
    "        \n",
    "        bpe_head_mask.append(0); upos_ids.append(-1); feat_ids.append(-1) # --> END token\n",
    "        bpe_head_mask.extend([0] * (self.max_len - len(bpe_head_mask))) ## --> padding by max_len\n",
    "        upos_ids.extend([-1] * (self.max_len - len(upos_ids))) ## --> padding by max_len\n",
    "        feat_ids.extend([-1] * (self.max_len - len(feat_ids))) ## --> padding by max_len\n",
    "        \n",
    "        return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'bpe_head_mask': torch.tensor(bpe_head_mask, dtype=torch.long),\n",
    "                'upos_ids': torch.tensor(upos_ids, dtype=torch.long),\n",
    "                'feat_ids': torch.tensor(feat_ids, dtype=torch.long)\n",
    "               }\n",
    "    \n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(total_pred, total_targ, noNER_idx):\n",
    "    \n",
    "    p = 0 # (retrived SB and real SB) / retrived SB  # The percentage of (the number of correct predictions) / (the number of predction that system predicts as B-SENT)\n",
    "    r = 0\n",
    "    f1= 0\n",
    "    \n",
    "    np_total_pred = np.array(total_pred)\n",
    "    np_total_tag = np.array(total_targ)\n",
    "    \n",
    "    #Get noPad\n",
    "    incidence_nopad = np.where(np_total_tag != -1) ## eliminate paddings\n",
    "    np_total_pred_nopad = np_total_pred[incidence_nopad]\n",
    "    np_total_tag_nopad = np_total_tag[incidence_nopad]\n",
    "    \n",
    "    \n",
    "    #precision\n",
    "    incidence_nopad_sb = np.where(np_total_pred_nopad != noNER_idx)\n",
    "    np_total_pred_nopad_sb = np_total_pred_nopad[incidence_nopad_sb]\n",
    "    np_total_tag_nopad_sb = np_total_tag_nopad[incidence_nopad_sb]\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_nopad_sb)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_nopad_sb==np_total_tag_nopad_sb) == True)\n",
    "    \n",
    "    '''\n",
    "    np_total_pred_incid = np_total_pred[incidence_p]\n",
    "    print(\"np_total_pred_incid\", np_total_pred_incid)\n",
    "    ids_sb_pred_p = np.where(np_total_pred_incid==1)\n",
    "    np_total_pred_p = np_total_pred_incid[ids_sb_pred_p]\n",
    "    np_total_tag_p = np_total_tag[ids_sb_pred_p]\n",
    "    \n",
    "    print(\"ids_sb_pred_p\", ids_sb_pred_p)\n",
    "    print(\"np_total_pred_p\", np_total_pred_p)\n",
    "    print(\"np_total_tag_p\", np_total_tag_p)\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_p)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_p==np_total_tag_p) == True)\n",
    "    '''\n",
    "    \n",
    "    print(\"count_correct_p\", count_correct_p)\n",
    "    print(\"count_active_tokens_p\", count_active_tokens_p)\n",
    "    \n",
    "    p = count_correct_p/count_active_tokens_p\n",
    "    print(\"precision:\", p)\n",
    "\n",
    "    \n",
    "    #recall\n",
    "    ids_sb_pred_r = np.where(np_total_tag_nopad != noNER_idx)\n",
    "    np_total_pred_r = np_total_pred_nopad[ids_sb_pred_r]\n",
    "    np_total_tag_r = np_total_tag_nopad[ids_sb_pred_r]\n",
    "    \n",
    "    #print(\"ids_sb_pred_r\", ids_sb_pred_r)\n",
    "    #print(\"np_total_pred_r\", np_total_pred_r)\n",
    "    #print(\"np_total_tag_r\", np_total_tag_r)\n",
    "    \n",
    "    count_active_tokens_r = len(np_total_pred_r)\n",
    "    count_correct_r = np.count_nonzero((np_total_pred_r==np_total_tag_r) == True)\n",
    "    \n",
    "    print(\"count_active_tokens_r\", count_active_tokens_r)\n",
    "    print(\"count_correct_r\", count_correct_r)\n",
    "    \n",
    "    r = count_correct_r/count_active_tokens_r\n",
    "    print(\"recall:\", r)\n",
    "    \n",
    "    \n",
    "    #F1\n",
    "    #f1 = 2*(p*r) / (p+r)\n",
    "    print(\"F1:\", f1)\n",
    "    \n",
    "    #count_active_tokens_recall = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    #print(\"count_active_tokens_recall\", count_active_tokens_recall)\n",
    "    #count_active_tokens_precision = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    \n",
    "    #count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"count_correct\",count_correct)\n",
    "    #print(\"ACCURACY:\", count_correct/count_active_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaEncoder(nn.Module):\n",
    "    def __init__(self, num_upos, num_feat):\n",
    "        super(XLMRobertaEncoder, self).__init__()\n",
    "        self.xlm_roberta = transformers.AutoModel.from_pretrained(BERT_MODEL) #transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear = nn.Linear(768, num_upos)\n",
    "        \n",
    "        self.f_dropout = nn.Dropout(0.33)\n",
    "        self.f_linear = nn.Linear(768, num_feat)\n",
    "            \n",
    "    def forward(self, ids, mask):\n",
    "        o1, o2 = self.xlm_roberta(ids, mask, return_dict=False)\n",
    "        \n",
    "        #apool = torch.mean(o1, 1)\n",
    "        #mpool, _ = torch.max(o1, 1)\n",
    "        #cat = torch.cat((apool, mpool), 1)\n",
    "        #bo = self.dropout(cat)\n",
    "        p_logits = self.linear(o1)        \n",
    "        f_logits = self.f_linear(o1)   \n",
    "        \n",
    "        return p_logits, f_logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_conll with ko\n",
      "Vocab containing 1504 words\n",
      "Charset containing 1539 chars\n",
      "UPOS containing 1 tags Counter({'_': 388005})\n",
      "Rels containing 1 tags Counter({'_': 388005})\n",
      "Feats containing 11 tags Counter({'o': 306949, 'i-og': 20055, 'i-ps': 17752, 'i-dt': 11734, 'b-ps': 7602, 'i-lc': 6638, 'b-og': 6347, 'b-dt': 4032, 'b-lc': 3931, 'i-ti': 2239, 'b-ti': 726})\n",
      "lang containing 1 tags Counter({'ko': 388005})\n"
     ]
    }
   ],
   "source": [
    "if VALID_FILE is not None:\n",
    "    train_graphs = read_conll(TRAIN_FILE, 'ko')\n",
    "    valid_graphs = read_conll(VALID_FILE, 'ko')\n",
    "else:\n",
    "    graphs = read_conll(TRAIN_FILE, 'ko')\n",
    "    if DATASET.startswith(\"KLUE\"):\n",
    "        valid_graphs = graphs[9001:18000]\n",
    "        train_graphs = graphs[18001:]\n",
    "    elif DATASET.startswith(\"MODU21\"):\n",
    "        valid_graphs = graphs[69485:]\n",
    "        train_graphs = graphs[:68400]\n",
    "    elif DATASET.startswith(\"MODU19\"):\n",
    "        valid_graphs = graphs[round(len(graphs)*0.8):]\n",
    "        train_graphs = graphs[:round(len(graphs)*0.8)]\n",
    "    elif DATASET.startswith(\"ETRI\"):\n",
    "        valid_graphs = graphs[round(len(graphs)*0.8):]\n",
    "        train_graphs = graphs[:round(len(graphs)*0.8)]\n",
    "    else:\n",
    "        print(\"Please set the dataset among [KLUE, MODU21, MODU19, ETRI, NAVER]\")\n",
    "\n",
    "train_dataset = CoNLLDataset(graphs=train_graphs, tokenizer=TOKENIZER, max_len=MAX_LEN)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=4, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "valid_dataset = CoNLLDataset(graphs=valid_graphs, tokenizer=TOKENIZER, max_len=MAX_LEN, fullvocab=train_dataset._fullvocab)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=4, batch_size=VALID_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "num_upos = len(train_dataset._upos)\n",
    "num_feat = len(train_dataset._feats)\n",
    "model = XLMRobertaEncoder(num_upos, num_feat)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "lr = 0.000005\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(train_loader, model, optimizer, DEVICE, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    p_total_pred = []\n",
    "    p_total_targ = []\n",
    "    p_total_loss = []\n",
    "    \n",
    "    f_total_pred = []\n",
    "    f_total_targ = []\n",
    "    f_total_loss = []\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        p_logits, f_logits = model(batch['ids'].cuda(), batch['mask'].cuda())\n",
    "        \n",
    "        #UPOS\n",
    "        b,s,l = p_logits.size()\n",
    "        #print(p_logits.view(b*s,l), p_logits.view(b*s,l).size())\n",
    "        #print(batch['upos_ids'].cuda().view(b*s), batch['upos_ids'].cuda().view(b*s).size())\n",
    "        p_loss = loss_fn(p_logits.view(b*s,l), batch['upos_ids'].cuda().view(b*s))\n",
    "        p_total_loss.append(p_loss.item())\n",
    "        p_total_pred.extend(torch.argmax(p_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        p_total_targ.extend(batch['upos_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        \n",
    "        #FEAT\n",
    "        b,s,l = f_logits.size()\n",
    "        f_loss = loss_fn(f_logits.view(b*s,l), batch['feat_ids'].cuda().view(b*s))\n",
    "        f_total_loss.append(f_loss.item())\n",
    "        f_total_pred.extend(torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        f_total_targ.extend(batch['feat_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        \n",
    "        #loss = p_loss+f_loss\n",
    "        loss = f_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    count_active_tokens = np.count_nonzero(np.array(p_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(p_total_pred)==np.array(p_total_targ)) == True)\n",
    "    print(\"TRAINING POS ACCURACY:\", count_correct/count_active_tokens)\n",
    "    \n",
    "    count_active_tokens = np.count_nonzero(np.array(f_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(f_total_pred)==np.array(f_total_targ)) == True)\n",
    "    f1_score(f_total_pred, f_total_targ, train_dataset._feats.get('o', 2))\n",
    "    print(\"TRAINING FEAT ACCURACY:\", count_correct/count_active_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loop_fn(dev_loader, model, DEVICE):\n",
    "    model.eval()\n",
    "    \n",
    "    p_total_pred = []\n",
    "    p_total_targ = []\n",
    "    p_total_loss = []\n",
    "    \n",
    "    f_total_pred = []\n",
    "    f_total_targ = []\n",
    "    f_total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(dev_loader), total=len(dev_loader)):\n",
    "\n",
    "            p_logits, f_logits = model(batch['ids'].cuda(), batch['mask'].cuda())\n",
    "\n",
    "            #UPOS\n",
    "            b,s,l = p_logits.size()\n",
    "            p_loss = loss_fn(p_logits.view(b*s,l), batch['upos_ids'].cuda().view(b*s))\n",
    "            p_total_loss.append(p_loss.item())\n",
    "            p_total_pred.extend(torch.argmax(p_logits.view(b*s,l), 1).cpu().tolist())\n",
    "            p_total_targ.extend(batch['upos_ids'].cuda().view(b*s).cpu().tolist())\n",
    "\n",
    "            #FEAT\n",
    "            b,s,l = f_logits.size()\n",
    "            f_loss = loss_fn(f_logits.view(b*s,l), batch['feat_ids'].cuda().view(b*s))\n",
    "            f_total_loss.append(f_loss.item())\n",
    "            f_total_pred.extend(torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "            f_total_targ.extend(batch['feat_ids'].cuda().view(b*s).cpu().tolist())\n",
    "\n",
    "            loss = p_loss+f_loss\n",
    "        \n",
    "    count_active_tokens = np.count_nonzero(np.array(p_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(p_total_pred)==np.array(p_total_targ)) == True)\n",
    "    print(\"VALIDATION POS ACCURACY:\", count_correct/count_active_tokens)\n",
    "    \n",
    "    count_active_tokens = np.count_nonzero(np.array(f_total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(f_total_pred)==np.array(f_total_targ)) == True)\n",
    "    f1_score(f_total_pred, f_total_targ, train_dataset._feats.get('o', 2))\n",
    "    print(\"VALIDATION FEAT ACCURACY:\", count_correct/count_active_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset._feats.get('o'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/351 [00:00<?, ?it/s]/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "\r  0%|          | 1/351 [00:03<23:05,  3.96s/it]\r  1%|          | 2/351 [00:07<23:09,  3.98s/it]\r  1%|          | 3/351 [00:11<21:33,  3.72s/it]\r  1%|          | 4/351 [00:14<20:46,  3.59s/it]\r  1%|▏         | 5/351 [00:18<20:22,  3.53s/it]\r  2%|▏         | 6/351 [00:21<20:04,  3.49s/it]\r  2%|▏         | 7/351 [00:25<19:57,  3.48s/it]\r  2%|▏         | 8/351 [00:28<19:45,  3.46s/it]\r  3%|▎         | 9/351 [00:31<19:35,  3.44s/it]\r  3%|▎         | 10/351 [00:35<19:30,  3.43s/it]\r  3%|▎         | 11/351 [00:38<19:23,  3.42s/it]\r  3%|▎         | 12/351 [00:42<19:18,  3.42s/it]\r  4%|▎         | 13/351 [00:45<19:13,  3.41s/it]\r  4%|▍         | 14/351 [00:48<19:12,  3.42s/it]\r  4%|▍         | 15/351 [00:52<19:07,  3.41s/it]\r  5%|▍         | 16/351 [00:55<19:02,  3.41s/it]\r  5%|▍         | 17/351 [00:59<18:58,  3.41s/it]\r  5%|▌         | 18/351 [01:02<18:54,  3.41s/it]\r  5%|▌         | 19/351 [01:05<18:52,  3.41s/it]\r  6%|▌         | 20/351 [01:09<18:54,  3.43s/it]\r  6%|▌         | 21/351 [01:12<18:49,  3.42s/it]\r  6%|▋         | 22/351 [01:16<18:58,  3.46s/it]\r  7%|▋         | 23/351 [01:19<18:48,  3.44s/it]\r  7%|▋         | 24/351 [01:23<18:47,  3.45s/it]\r  7%|▋         | 25/351 [01:26<18:39,  3.43s/it]\r  7%|▋         | 26/351 [01:30<18:32,  3.42s/it]\r  8%|▊         | 27/351 [01:33<18:27,  3.42s/it]\r  8%|▊         | 28/351 [01:36<18:28,  3.43s/it]\r  8%|▊         | 29/351 [01:40<18:33,  3.46s/it]\r  9%|▊         | 30/351 [01:43<18:24,  3.44s/it]\r  9%|▉         | 31/351 [01:47<18:16,  3.43s/it]\r  9%|▉         | 32/351 [01:50<18:10,  3.42s/it]\r  9%|▉         | 33/351 [01:54<18:04,  3.41s/it]\r 10%|▉         | 34/351 [01:57<18:00,  3.41s/it]\r 10%|▉         | 35/351 [02:00<17:56,  3.41s/it]\r 10%|█         | 36/351 [02:04<17:51,  3.40s/it]\r 11%|█         | 37/351 [02:07<17:57,  3.43s/it]\r 11%|█         | 38/351 [02:11<17:53,  3.43s/it]\r 11%|█         | 39/351 [02:14<17:47,  3.42s/it]\r 11%|█▏        | 40/351 [02:17<17:41,  3.41s/it]\r 12%|█▏        | 41/351 [02:21<17:36,  3.41s/it]\r 12%|█▏        | 42/351 [02:24<17:32,  3.41s/it]\r 12%|█▏        | 43/351 [02:28<17:30,  3.41s/it]\r 13%|█▎        | 44/351 [02:31<17:25,  3.41s/it]\r 13%|█▎        | 45/351 [02:34<17:21,  3.40s/it]\r 13%|█▎        | 46/351 [02:38<17:25,  3.43s/it]\r 13%|█▎        | 47/351 [02:41<17:19,  3.42s/it]\r 14%|█▎        | 48/351 [02:45<17:14,  3.41s/it]\r 14%|█▍        | 49/351 [02:48<17:11,  3.42s/it]\r 14%|█▍        | 50/351 [02:52<17:06,  3.41s/it]\r 15%|█▍        | 51/351 [02:55<17:07,  3.43s/it]\r 15%|█▍        | 52/351 [02:59<17:16,  3.47s/it]\r 15%|█▌        | 53/351 [03:02<17:07,  3.45s/it]\r 15%|█▌        | 54/351 [03:05<17:05,  3.45s/it]\r 16%|█▌        | 55/351 [03:09<16:56,  3.44s/it]\r 16%|█▌        | 56/351 [03:12<16:52,  3.43s/it]\r 16%|█▌        | 57/351 [03:16<16:46,  3.42s/it]\r 17%|█▋        | 58/351 [03:19<16:40,  3.42s/it]\r 17%|█▋        | 59/351 [03:22<16:35,  3.41s/it]\r 17%|█▋        | 60/351 [03:26<16:35,  3.42s/it]\r 17%|█▋        | 61/351 [03:29<16:30,  3.41s/it]\r 18%|█▊        | 62/351 [03:33<16:25,  3.41s/it]\r 18%|█▊        | 63/351 [03:36<16:25,  3.42s/it]\r 18%|█▊        | 64/351 [03:40<16:21,  3.42s/it]\r 19%|█▊        | 65/351 [03:43<16:15,  3.41s/it]\r 19%|█▉        | 66/351 [03:46<16:15,  3.42s/it]\r 19%|█▉        | 67/351 [03:50<16:11,  3.42s/it]\r 19%|█▉        | 68/351 [03:53<16:05,  3.41s/it]\r 20%|█▉        | 69/351 [03:57<16:00,  3.41s/it]\r 20%|█▉        | 70/351 [04:00<16:00,  3.42s/it]\r 20%|██        | 71/351 [04:03<15:54,  3.41s/it]\r 21%|██        | 72/351 [04:07<15:58,  3.44s/it]\r 21%|██        | 73/351 [04:10<15:53,  3.43s/it]\r 21%|██        | 74/351 [04:14<15:47,  3.42s/it]\r 21%|██▏       | 75/351 [04:17<15:48,  3.44s/it]\r 22%|██▏       | 76/351 [04:21<15:43,  3.43s/it]\r 22%|██▏       | 77/351 [04:24<15:37,  3.42s/it]\r 22%|██▏       | 78/351 [04:28<15:40,  3.45s/it]\r 23%|██▎       | 79/351 [04:31<15:33,  3.43s/it]\r 23%|██▎       | 80/351 [04:34<15:27,  3.42s/it]\r 23%|██▎       | 81/351 [04:38<15:21,  3.41s/it]\r 23%|██▎       | 82/351 [04:41<15:16,  3.41s/it]\r 24%|██▎       | 83/351 [04:45<15:13,  3.41s/it]\r 24%|██▍       | 84/351 [04:48<15:09,  3.41s/it]\r 24%|██▍       | 85/351 [04:51<15:05,  3.40s/it]\r 25%|██▍       | 86/351 [04:55<15:01,  3.40s/it]\r 25%|██▍       | 87/351 [04:58<14:58,  3.40s/it]\r 25%|██▌       | 88/351 [05:02<14:57,  3.41s/it]\r 25%|██▌       | 89/351 [05:05<14:53,  3.41s/it]\r 26%|██▌       | 90/351 [05:08<14:48,  3.40s/it]\r 26%|██▌       | 91/351 [05:12<14:47,  3.41s/it]\r 26%|██▌       | 92/351 [05:15<14:43,  3.41s/it]\r 26%|██▋       | 93/351 [05:19<14:38,  3.41s/it]\r 27%|██▋       | 94/351 [05:22<14:37,  3.41s/it]\r 27%|██▋       | 95/351 [05:25<14:32,  3.41s/it]\r 27%|██▋       | 96/351 [05:29<14:28,  3.40s/it]\r 28%|██▊       | 97/351 [05:32<14:24,  3.40s/it]\r 28%|██▊       | 98/351 [05:36<14:22,  3.41s/it]\r 28%|██▊       | 99/351 [05:39<14:18,  3.41s/it]\r 28%|██▊       | 100/351 [05:42<14:13,  3.40s/it]\r 29%|██▉       | 101/351 [05:46<14:10,  3.40s/it]\r 29%|██▉       | 102/351 [05:49<14:06,  3.40s/it]\r 29%|██▉       | 103/351 [05:53<14:03,  3.40s/it]\r 30%|██▉       | 104/351 [05:56<13:59,  3.40s/it]\r 30%|██▉       | 105/351 [05:59<13:57,  3.40s/it]\r 30%|███       | 106/351 [06:03<13:55,  3.41s/it]\r 30%|███       | 107/351 [06:06<13:51,  3.41s/it]\r 31%|███       | 108/351 [06:10<13:50,  3.42s/it]\r 31%|███       | 109/351 [06:13<13:45,  3.41s/it]\r 31%|███▏      | 110/351 [06:17<13:47,  3.44s/it]\r 32%|███▏      | 111/351 [06:20<13:46,  3.44s/it]\r 32%|███▏      | 112/351 [06:23<13:39,  3.43s/it]\r 32%|███▏      | 113/351 [06:27<13:33,  3.42s/it]\r 32%|███▏      | 114/351 [06:30<13:33,  3.43s/it]\r 33%|███▎      | 115/351 [06:34<13:27,  3.42s/it]\r 33%|███▎      | 116/351 [06:37<13:22,  3.42s/it]\r 33%|███▎      | 117/351 [06:41<13:17,  3.41s/it]\r 34%|███▎      | 118/351 [06:44<13:13,  3.41s/it]\r 34%|███▍      | 119/351 [06:47<13:09,  3.40s/it]\r 34%|███▍      | 120/351 [06:51<13:10,  3.42s/it]\r 34%|███▍      | 121/351 [06:54<13:05,  3.42s/it]\r 35%|███▍      | 122/351 [06:58<13:00,  3.41s/it]\r 35%|███▌      | 123/351 [07:01<12:56,  3.41s/it]\r 35%|███▌      | 124/351 [07:04<12:52,  3.40s/it]\r 36%|███▌      | 125/351 [07:08<12:57,  3.44s/it]\r 36%|███▌      | 126/351 [07:11<12:51,  3.43s/it]\r 36%|███▌      | 127/351 [07:15<12:45,  3.42s/it]\r 36%|███▋      | 128/351 [07:18<12:40,  3.41s/it]\r 37%|███▋      | 129/351 [07:21<12:36,  3.41s/it]\r 37%|███▋      | 130/351 [07:25<12:34,  3.41s/it]\r 37%|███▋      | 131/351 [07:28<12:30,  3.41s/it]\r 38%|███▊      | 132/351 [07:32<12:26,  3.41s/it]\r 38%|███▊      | 133/351 [07:35<12:26,  3.43s/it]\r 38%|███▊      | 134/351 [07:39<12:21,  3.42s/it]\r 38%|███▊      | 135/351 [07:42<12:22,  3.44s/it]\r 39%|███▊      | 136/351 [07:45<12:17,  3.43s/it]\r 39%|███▉      | 137/351 [07:49<12:12,  3.42s/it]\r 39%|███▉      | 138/351 [07:52<12:07,  3.41s/it]\r 40%|███▉      | 139/351 [07:56<12:04,  3.42s/it]\r 40%|███▉      | 140/351 [07:59<12:00,  3.41s/it]\r 40%|████      | 141/351 [08:02<11:55,  3.41s/it]\r 40%|████      | 142/351 [08:06<11:51,  3.41s/it]\r 41%|████      | 143/351 [08:09<11:47,  3.40s/it]\r 41%|████      | 144/351 [08:13<11:44,  3.40s/it]\r 41%|████▏     | 145/351 [08:16<11:42,  3.41s/it]\r 42%|████▏     | 146/351 [08:20<11:38,  3.41s/it]\r 42%|████▏     | 147/351 [08:23<11:34,  3.40s/it]\r 42%|████▏     | 148/351 [08:26<11:30,  3.40s/it]\r 42%|████▏     | 149/351 [08:30<11:26,  3.40s/it]\r 43%|████▎     | 150/351 [08:33<11:23,  3.40s/it]\r 43%|████▎     | 151/351 [08:37<11:21,  3.41s/it]\r 43%|████▎     | 152/351 [08:40<11:17,  3.40s/it]\r 44%|████▎     | 153/351 [08:43<11:13,  3.40s/it]\r 44%|████▍     | 154/351 [08:47<11:14,  3.42s/it]\r 44%|████▍     | 155/351 [08:50<11:09,  3.42s/it]\r 44%|████▍     | 156/351 [08:54<11:05,  3.41s/it]\r 45%|████▍     | 157/351 [08:57<11:01,  3.41s/it]\r 45%|████▌     | 158/351 [09:00<11:02,  3.43s/it]\r 45%|████▌     | 159/351 [09:04<10:57,  3.42s/it]\r 46%|████▌     | 160/351 [09:07<10:52,  3.41s/it]\r 46%|████▌     | 161/351 [09:11<10:47,  3.41s/it]\r 46%|████▌     | 162/351 [09:14<10:43,  3.41s/it]\r 46%|████▋     | 163/351 [09:17<10:39,  3.40s/it]\r 47%|████▋     | 164/351 [09:21<10:36,  3.40s/it]\r 47%|████▋     | 165/351 [09:24<10:32,  3.40s/it]\r 47%|████▋     | 166/351 [09:28<10:29,  3.40s/it]\r 48%|████▊     | 167/351 [09:31<10:25,  3.40s/it]\r 48%|████▊     | 168/351 [09:34<10:22,  3.40s/it]\r 48%|████▊     | 169/351 [09:38<10:19,  3.40s/it]\r 48%|████▊     | 170/351 [09:41<10:15,  3.40s/it]\r 49%|████▊     | 171/351 [09:45<10:12,  3.40s/it]\r 49%|████▉     | 172/351 [09:48<10:12,  3.42s/it]\r 49%|████▉     | 173/351 [09:52<10:07,  3.41s/it]\r 50%|████▉     | 174/351 [09:55<10:05,  3.42s/it]\r 50%|████▉     | 175/351 [09:58<10:03,  3.43s/it]\r 50%|█████     | 176/351 [10:02<10:03,  3.45s/it]\r 50%|█████     | 177/351 [10:05<09:58,  3.44s/it]\r 51%|█████     | 178/351 [10:09<09:52,  3.43s/it]\r 51%|█████     | 179/351 [10:12<09:49,  3.43s/it]\r 51%|█████▏    | 180/351 [10:16<09:45,  3.43s/it]\r 52%|█████▏    | 181/351 [10:19<09:41,  3.42s/it]\r 52%|█████▏    | 182/351 [10:22<09:36,  3.41s/it]\r 52%|█████▏    | 183/351 [10:26<09:32,  3.41s/it]\r 52%|█████▏    | 184/351 [10:29<09:28,  3.40s/it]\r 53%|█████▎    | 185/351 [10:33<09:24,  3.40s/it]\r 53%|█████▎    | 186/351 [10:36<09:22,  3.41s/it]\r 53%|█████▎    | 187/351 [10:39<09:18,  3.40s/it]\r 54%|█████▎    | 188/351 [10:43<09:14,  3.40s/it]\r 54%|█████▍    | 189/351 [10:46<09:10,  3.40s/it]\r 54%|█████▍    | 190/351 [10:50<09:07,  3.40s/it]\r 54%|█████▍    | 191/351 [10:53<09:03,  3.40s/it]\r 55%|█████▍    | 192/351 [10:56<09:00,  3.40s/it]\r 55%|█████▍    | 193/351 [11:00<08:56,  3.40s/it]\r 55%|█████▌    | 194/351 [11:03<08:53,  3.40s/it]\r 56%|█████▌    | 195/351 [11:07<08:52,  3.41s/it]\r 56%|█████▌    | 196/351 [11:10<08:51,  3.43s/it]\r 56%|█████▌    | 197/351 [11:13<08:46,  3.42s/it]\r 56%|█████▋    | 198/351 [11:17<08:44,  3.43s/it]\r 57%|█████▋    | 199/351 [11:20<08:39,  3.42s/it]\r 57%|█████▋    | 200/351 [11:24<08:34,  3.41s/it]\r 57%|█████▋    | 201/351 [11:27<08:32,  3.41s/it]\r 58%|█████▊    | 202/351 [11:31<08:28,  3.41s/it]\r 58%|█████▊    | 203/351 [11:34<08:24,  3.41s/it]\r 58%|█████▊    | 204/351 [11:37<08:20,  3.40s/it]\r 58%|█████▊    | 205/351 [11:41<08:16,  3.40s/it]\r 59%|█████▊    | 206/351 [11:44<08:13,  3.40s/it]\r 59%|█████▉    | 207/351 [11:48<08:13,  3.43s/it]\r 59%|█████▉    | 208/351 [11:51<08:08,  3.42s/it]\r 60%|█████▉    | 209/351 [11:54<08:04,  3.41s/it]\r 60%|█████▉    | 210/351 [11:58<08:00,  3.41s/it]\r 60%|██████    | 211/351 [12:01<08:00,  3.44s/it]\r 60%|██████    | 212/351 [12:05<07:55,  3.42s/it]\r 61%|██████    | 213/351 [12:08<07:51,  3.41s/it]\r 61%|██████    | 214/351 [12:12<07:48,  3.42s/it]\r 61%|██████▏   | 215/351 [12:15<07:44,  3.41s/it]\r 62%|██████▏   | 216/351 [12:18<07:40,  3.41s/it]\r 62%|██████▏   | 217/351 [12:22<07:38,  3.42s/it]\r 62%|██████▏   | 218/351 [12:25<07:33,  3.41s/it]\r 62%|██████▏   | 219/351 [12:29<07:30,  3.41s/it]\r 63%|██████▎   | 220/351 [12:32<07:28,  3.42s/it]\r 63%|██████▎   | 221/351 [12:36<07:28,  3.45s/it]\r 63%|██████▎   | 222/351 [12:39<07:22,  3.43s/it]\r 64%|██████▎   | 223/351 [12:42<07:22,  3.46s/it]\r 64%|██████▍   | 224/351 [12:46<07:16,  3.44s/it]\r 64%|██████▍   | 225/351 [12:49<07:12,  3.43s/it]\r 64%|██████▍   | 226/351 [12:53<07:09,  3.44s/it]\r 65%|██████▍   | 227/351 [12:56<07:04,  3.42s/it]\r 65%|██████▍   | 228/351 [13:00<07:00,  3.42s/it]\r 65%|██████▌   | 229/351 [13:03<07:00,  3.45s/it]\r 66%|██████▌   | 230/351 [13:06<06:55,  3.43s/it]\r 66%|██████▌   | 231/351 [13:10<06:51,  3.43s/it]\r 66%|██████▌   | 232/351 [13:13<06:47,  3.42s/it]\r 66%|██████▋   | 233/351 [13:17<06:42,  3.41s/it]\r 67%|██████▋   | 234/351 [13:20<06:38,  3.41s/it]\r 67%|██████▋   | 235/351 [13:23<06:34,  3.40s/it]\r 67%|██████▋   | 236/351 [13:27<06:31,  3.40s/it]\r 68%|██████▊   | 237/351 [13:30<06:28,  3.41s/it]\r 68%|██████▊   | 238/351 [13:34<06:24,  3.41s/it]\r 68%|██████▊   | 239/351 [13:37<06:22,  3.41s/it]\r 68%|██████▊   | 240/351 [13:40<06:18,  3.41s/it]\r 69%|██████▊   | 241/351 [13:44<06:15,  3.42s/it]\r 69%|██████▉   | 242/351 [13:48<06:18,  3.47s/it]\r 69%|██████▉   | 243/351 [13:51<06:12,  3.45s/it]\r 70%|██████▉   | 244/351 [13:54<06:07,  3.43s/it]\r 70%|██████▉   | 245/351 [13:58<06:02,  3.42s/it]\r 70%|███████   | 246/351 [14:01<05:58,  3.42s/it]\r 70%|███████   | 247/351 [14:05<05:54,  3.41s/it]\r 71%|███████   | 248/351 [14:08<05:51,  3.41s/it]\r 71%|███████   | 249/351 [14:11<05:49,  3.42s/it]\r 71%|███████   | 250/351 [14:15<05:45,  3.42s/it]\r 72%|███████▏  | 251/351 [14:18<05:41,  3.41s/it]\r 72%|███████▏  | 252/351 [14:22<05:37,  3.41s/it]\r 72%|███████▏  | 253/351 [14:25<05:33,  3.41s/it]\r 72%|███████▏  | 254/351 [14:28<05:30,  3.40s/it]\r 73%|███████▎  | 255/351 [14:32<05:26,  3.40s/it]\r 73%|███████▎  | 256/351 [14:35<05:23,  3.41s/it]\r 73%|███████▎  | 257/351 [14:39<05:19,  3.40s/it]\r 74%|███████▎  | 258/351 [14:42<05:16,  3.40s/it]\r 74%|███████▍  | 259/351 [14:45<05:12,  3.40s/it]\r 74%|███████▍  | 260/351 [14:49<05:10,  3.41s/it]\r 74%|███████▍  | 261/351 [14:52<05:06,  3.41s/it]\r 75%|███████▍  | 262/351 [14:56<05:03,  3.41s/it]\r 75%|███████▍  | 263/351 [14:59<05:00,  3.41s/it]\r 75%|███████▌  | 264/351 [15:02<04:56,  3.41s/it]\r 75%|███████▌  | 265/351 [15:06<04:52,  3.41s/it]\r 76%|███████▌  | 266/351 [15:09<04:50,  3.42s/it]\r 76%|███████▌  | 267/351 [15:13<04:46,  3.41s/it]\r 76%|███████▋  | 268/351 [15:16<04:42,  3.41s/it]\r 77%|███████▋  | 269/351 [15:19<04:39,  3.40s/it]\r 77%|███████▋  | 270/351 [15:23<04:35,  3.40s/it]\r 77%|███████▋  | 271/351 [15:26<04:32,  3.40s/it]\r 77%|███████▋  | 272/351 [15:30<04:28,  3.40s/it]\r 78%|███████▊  | 273/351 [15:33<04:25,  3.40s/it]\r 78%|███████▊  | 274/351 [15:36<04:22,  3.41s/it]\r 78%|███████▊  | 275/351 [15:40<04:18,  3.41s/it]\r 79%|███████▊  | 276/351 [15:43<04:15,  3.40s/it]\r 79%|███████▉  | 277/351 [15:47<04:11,  3.40s/it]\r 79%|███████▉  | 278/351 [15:50<04:08,  3.40s/it]\r 79%|███████▉  | 279/351 [15:53<04:04,  3.40s/it]\r 80%|███████▉  | 280/351 [15:57<04:01,  3.40s/it]\r 80%|████████  | 281/351 [16:00<03:57,  3.40s/it]\r 80%|████████  | 282/351 [16:04<03:54,  3.40s/it]\r 81%|████████  | 283/351 [16:07<03:53,  3.43s/it]\r 81%|████████  | 284/351 [16:11<03:49,  3.42s/it]\r 81%|████████  | 285/351 [16:14<03:45,  3.41s/it]\r 81%|████████▏ | 286/351 [16:17<03:42,  3.42s/it]\r 82%|████████▏ | 287/351 [16:21<03:38,  3.41s/it]\r 82%|████████▏ | 288/351 [16:24<03:34,  3.41s/it]\r 82%|████████▏ | 289/351 [16:28<03:31,  3.41s/it]\r 83%|████████▎ | 290/351 [16:31<03:27,  3.40s/it]\r 83%|████████▎ | 291/351 [16:34<03:23,  3.40s/it]\r 83%|████████▎ | 292/351 [16:38<03:20,  3.40s/it]\r 83%|████████▎ | 293/351 [16:41<03:17,  3.40s/it]\r 84%|████████▍ | 294/351 [16:45<03:13,  3.40s/it]\r 84%|████████▍ | 295/351 [16:48<03:10,  3.40s/it]\r 84%|████████▍ | 296/351 [16:52<03:13,  3.51s/it]\r 85%|████████▍ | 297/351 [16:55<03:07,  3.48s/it]\r 85%|████████▍ | 298/351 [16:59<03:03,  3.46s/it]\r 85%|████████▌ | 299/351 [17:02<02:58,  3.44s/it]\r 85%|████████▌ | 300/351 [17:05<02:54,  3.43s/it]\r 86%|████████▌ | 301/351 [17:09<02:50,  3.42s/it]\r 86%|████████▌ | 302/351 [17:12<02:47,  3.42s/it]\r 86%|████████▋ | 303/351 [17:16<02:43,  3.41s/it]\r 87%|████████▋ | 304/351 [17:19<02:42,  3.46s/it]\r 87%|████████▋ | 305/351 [17:23<02:38,  3.44s/it]\r 87%|████████▋ | 306/351 [17:26<02:34,  3.43s/it]\r 87%|████████▋ | 307/351 [17:29<02:30,  3.42s/it]\r 88%|████████▊ | 308/351 [17:33<02:26,  3.41s/it]\r 88%|████████▊ | 309/351 [17:36<02:23,  3.41s/it]\r 88%|████████▊ | 310/351 [17:40<02:19,  3.41s/it]\r 89%|████████▊ | 311/351 [17:43<02:16,  3.40s/it]\r 89%|████████▉ | 312/351 [17:46<02:12,  3.41s/it]\r 89%|████████▉ | 313/351 [17:50<02:09,  3.40s/it]\r 89%|████████▉ | 314/351 [17:53<02:05,  3.40s/it]\r 90%|████████▉ | 315/351 [17:57<02:02,  3.40s/it]\r 90%|█████████ | 316/351 [18:00<01:59,  3.41s/it]\r 90%|█████████ | 317/351 [18:03<01:56,  3.42s/it]\r 91%|█████████ | 318/351 [18:07<01:52,  3.41s/it]\r 91%|█████████ | 319/351 [18:10<01:49,  3.41s/it]\r 91%|█████████ | 320/351 [18:14<01:45,  3.41s/it]\r 91%|█████████▏| 321/351 [18:17<01:42,  3.41s/it]\r 92%|█████████▏| 322/351 [18:20<01:38,  3.41s/it]\r 92%|█████████▏| 323/351 [18:24<01:35,  3.42s/it]\r 92%|█████████▏| 324/351 [18:27<01:32,  3.41s/it]\r 93%|█████████▎| 325/351 [18:31<01:29,  3.45s/it]\r 93%|█████████▎| 326/351 [18:34<01:25,  3.43s/it]\r 93%|█████████▎| 327/351 [18:38<01:22,  3.42s/it]\r 93%|█████████▎| 328/351 [18:41<01:18,  3.41s/it]\r 94%|█████████▎| 329/351 [18:44<01:15,  3.41s/it]\r 94%|█████████▍| 330/351 [18:48<01:11,  3.41s/it]\r 94%|█████████▍| 331/351 [18:51<01:08,  3.40s/it]\r 95%|█████████▍| 332/351 [18:55<01:04,  3.41s/it]\r 95%|█████████▍| 333/351 [18:58<01:01,  3.41s/it]\r 95%|█████████▌| 334/351 [19:01<00:57,  3.40s/it]\r 95%|█████████▌| 335/351 [19:05<00:54,  3.41s/it]\r 96%|█████████▌| 336/351 [19:08<00:51,  3.41s/it]\r 96%|█████████▌| 337/351 [19:12<00:47,  3.40s/it]\r 96%|█████████▋| 338/351 [19:15<00:44,  3.41s/it]\r 97%|█████████▋| 339/351 [19:18<00:40,  3.40s/it]\r 97%|█████████▋| 340/351 [19:22<00:37,  3.40s/it]\r 97%|█████████▋| 341/351 [19:25<00:34,  3.40s/it]\r 97%|█████████▋| 342/351 [19:29<00:30,  3.40s/it]\r 98%|█████████▊| 343/351 [19:32<00:27,  3.40s/it]\r 98%|█████████▊| 344/351 [19:35<00:23,  3.40s/it]\r 98%|█████████▊| 345/351 [19:39<00:20,  3.40s/it]\r 99%|█████████▊| 346/351 [19:42<00:16,  3.40s/it]\r 99%|█████████▉| 347/351 [19:46<00:13,  3.40s/it]\r 99%|█████████▉| 348/351 [19:49<00:10,  3.41s/it]\r 99%|█████████▉| 349/351 [19:52<00:06,  3.40s/it]\r100%|█████████▉| 350/351 [19:56<00:03,  3.40s/it]\r100%|██████████| 351/351 [19:56<00:00,  2.54s/it]\r100%|██████████| 351/351 [19:57<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING POS ACCURACY: 0.8209902486085848\n",
      "count_correct_p 33713\n",
      "count_active_tokens_p 51028\n",
      "precision: 0.6606764913380889\n",
      "count_active_tokens_r 84556\n",
      "count_correct_r 33713\n",
      "recall: 0.39870618288471543\n",
      "F1: 0\n",
      "TRAINING FEAT ACCURACY: 0.9052323335789189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "\r  1%|          | 1/84 [00:01<01:47,  1.30s/it]\r  2%|▏         | 2/84 [00:02<01:40,  1.23s/it]\r  4%|▎         | 3/84 [00:03<01:38,  1.21s/it]\r  5%|▍         | 4/84 [00:04<01:35,  1.20s/it]\r  6%|▌         | 5/84 [00:06<01:34,  1.20s/it]\r  7%|▋         | 6/84 [00:07<01:32,  1.19s/it]\r  8%|▊         | 7/84 [00:08<01:31,  1.19s/it]\r 10%|▉         | 8/84 [00:09<01:30,  1.19s/it]\r 11%|█         | 9/84 [00:10<01:29,  1.19s/it]\r 12%|█▏        | 10/84 [00:11<01:27,  1.19s/it]\r 13%|█▎        | 11/84 [00:13<01:26,  1.19s/it]\r 14%|█▍        | 12/84 [00:14<01:25,  1.19s/it]\r 15%|█▌        | 13/84 [00:15<01:24,  1.19s/it]\r 17%|█▋        | 14/84 [00:16<01:23,  1.19s/it]\r 18%|█▊        | 15/84 [00:17<01:22,  1.19s/it]\r 19%|█▉        | 16/84 [00:19<01:20,  1.19s/it]\r 20%|██        | 17/84 [00:20<01:19,  1.19s/it]\r 21%|██▏       | 18/84 [00:21<01:18,  1.19s/it]\r 23%|██▎       | 19/84 [00:22<01:17,  1.19s/it]\r 24%|██▍       | 20/84 [00:23<01:16,  1.19s/it]\r 25%|██▌       | 21/84 [00:25<01:14,  1.19s/it]\r 26%|██▌       | 22/84 [00:26<01:13,  1.19s/it]\r 27%|██▋       | 23/84 [00:27<01:12,  1.19s/it]\r 29%|██▊       | 24/84 [00:28<01:11,  1.19s/it]\r 30%|██▉       | 25/84 [00:29<01:09,  1.19s/it]\r 31%|███       | 26/84 [00:30<01:08,  1.19s/it]\r 32%|███▏      | 27/84 [00:32<01:07,  1.19s/it]\r 33%|███▎      | 28/84 [00:33<01:06,  1.18s/it]\r 35%|███▍      | 29/84 [00:34<01:05,  1.19s/it]\r 36%|███▌      | 30/84 [00:35<01:04,  1.19s/it]\r 37%|███▋      | 31/84 [00:36<01:02,  1.19s/it]\r 38%|███▊      | 32/84 [00:38<01:01,  1.19s/it]\r 39%|███▉      | 33/84 [00:39<01:00,  1.19s/it]\r 40%|████      | 34/84 [00:40<00:59,  1.18s/it]\r 42%|████▏     | 35/84 [00:41<00:58,  1.18s/it]\r 43%|████▎     | 36/84 [00:42<00:56,  1.18s/it]\r 44%|████▍     | 37/84 [00:44<00:55,  1.18s/it]\r 45%|████▌     | 38/84 [00:45<00:54,  1.19s/it]\r 46%|████▋     | 39/84 [00:46<00:53,  1.19s/it]\r 48%|████▊     | 40/84 [00:47<00:52,  1.19s/it]\r 49%|████▉     | 41/84 [00:48<00:51,  1.19s/it]\r 50%|█████     | 42/84 [00:49<00:49,  1.19s/it]\r 51%|█████     | 43/84 [00:51<00:48,  1.19s/it]\r 52%|█████▏    | 44/84 [00:52<00:47,  1.19s/it]\r 54%|█████▎    | 45/84 [00:53<00:46,  1.19s/it]\r 55%|█████▍    | 46/84 [00:54<00:45,  1.19s/it]\r 56%|█████▌    | 47/84 [00:55<00:43,  1.19s/it]\r 57%|█████▋    | 48/84 [00:57<00:42,  1.19s/it]\r 58%|█████▊    | 49/84 [00:58<00:41,  1.20s/it]\r 60%|█████▉    | 50/84 [00:59<00:40,  1.19s/it]\r 61%|██████    | 51/84 [01:00<00:39,  1.19s/it]\r 62%|██████▏   | 52/84 [01:01<00:38,  1.19s/it]\r 63%|██████▎   | 53/84 [01:03<00:36,  1.19s/it]\r 64%|██████▍   | 54/84 [01:04<00:35,  1.19s/it]\r 65%|██████▌   | 55/84 [01:05<00:34,  1.19s/it]\r 67%|██████▋   | 56/84 [01:06<00:33,  1.20s/it]\r 68%|██████▊   | 57/84 [01:07<00:32,  1.19s/it]\r 69%|██████▉   | 58/84 [01:09<00:30,  1.19s/it]\r 70%|███████   | 59/84 [01:10<00:29,  1.19s/it]\r 71%|███████▏  | 60/84 [01:11<00:28,  1.19s/it]\r 73%|███████▎  | 61/84 [01:12<00:27,  1.19s/it]\r 74%|███████▍  | 62/84 [01:13<00:26,  1.18s/it]\r 75%|███████▌  | 63/84 [01:14<00:24,  1.18s/it]\r 76%|███████▌  | 64/84 [01:16<00:23,  1.19s/it]\r 77%|███████▋  | 65/84 [01:17<00:22,  1.19s/it]\r 79%|███████▊  | 66/84 [01:18<00:21,  1.19s/it]\r 80%|███████▉  | 67/84 [01:19<00:20,  1.19s/it]\r 81%|████████  | 68/84 [01:20<00:19,  1.19s/it]\r 82%|████████▏ | 69/84 [01:22<00:17,  1.19s/it]\r 83%|████████▎ | 70/84 [01:23<00:16,  1.19s/it]\r 85%|████████▍ | 71/84 [01:24<00:15,  1.19s/it]\r 86%|████████▌ | 72/84 [01:25<00:14,  1.19s/it]\r 87%|████████▋ | 73/84 [01:26<00:13,  1.19s/it]\r 88%|████████▊ | 74/84 [01:28<00:11,  1.19s/it]\r 89%|████████▉ | 75/84 [01:29<00:10,  1.19s/it]\r 90%|█████████ | 76/84 [01:30<00:09,  1.19s/it]\r 92%|█████████▏| 77/84 [01:31<00:08,  1.19s/it]\r 93%|█████████▎| 78/84 [01:32<00:07,  1.19s/it]\r 94%|█████████▍| 79/84 [01:33<00:05,  1.19s/it]\r 95%|█████████▌| 80/84 [01:35<00:04,  1.19s/it]\r 96%|█████████▋| 81/84 [01:36<00:03,  1.19s/it]\r 98%|█████████▊| 82/84 [01:37<00:02,  1.19s/it]\r 99%|█████████▉| 83/84 [01:38<00:01,  1.19s/it]\r100%|██████████| 84/84 [01:39<00:00,  1.03it/s]\r100%|██████████| 84/84 [01:39<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION POS ACCURACY: 0.943528792839689\n",
      "count_correct_p 17500\n",
      "count_active_tokens_p 23377\n",
      "precision: 0.7485990503486333\n",
      "count_active_tokens_r 25013\n",
      "count_correct_r 17500\n",
      "recall: 0.6996361891816255\n",
      "F1: 0\n",
      "VALIDATION FEAT ACCURACY: 0.9408360618289804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/351 [00:00<?, ?it/s]/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "\r  0%|          | 1/351 [00:03<20:50,  3.57s/it]\r  1%|          | 2/351 [00:06<20:11,  3.47s/it]\r  1%|          | 3/351 [00:10<20:16,  3.50s/it]\r  1%|          | 4/351 [00:13<19:58,  3.45s/it]\r  1%|▏         | 5/351 [00:17<19:49,  3.44s/it]\r  2%|▏         | 6/351 [00:20<19:47,  3.44s/it]\r  2%|▏         | 7/351 [00:24<19:38,  3.43s/it]\r  2%|▏         | 8/351 [00:27<19:32,  3.42s/it]\r  3%|▎         | 9/351 [00:30<19:32,  3.43s/it]\r  3%|▎         | 10/351 [00:34<19:26,  3.42s/it]\r  3%|▎         | 11/351 [00:37<19:20,  3.41s/it]\r  3%|▎         | 12/351 [00:41<19:16,  3.41s/it]\r  4%|▎         | 13/351 [00:44<19:11,  3.41s/it]\r  4%|▍         | 14/351 [00:48<19:09,  3.41s/it]\r  4%|▍         | 15/351 [00:51<19:20,  3.45s/it]\r  5%|▍         | 16/351 [00:54<19:11,  3.44s/it]\r  5%|▍         | 17/351 [00:58<19:04,  3.43s/it]\r  5%|▌         | 18/351 [01:01<18:59,  3.42s/it]\r  5%|▌         | 19/351 [01:05<18:54,  3.42s/it]\r  6%|▌         | 20/351 [01:08<18:49,  3.41s/it]\r  6%|▌         | 21/351 [01:12<18:53,  3.44s/it]\r  6%|▋         | 22/351 [01:15<18:47,  3.43s/it]\r  7%|▋         | 23/351 [01:18<18:41,  3.42s/it]\r  7%|▋         | 24/351 [01:22<18:47,  3.45s/it]\r  7%|▋         | 25/351 [01:25<18:39,  3.43s/it]\r  7%|▋         | 26/351 [01:29<18:32,  3.42s/it]\r  8%|▊         | 27/351 [01:32<18:31,  3.43s/it]\r  8%|▊         | 28/351 [01:36<18:25,  3.42s/it]\r  8%|▊         | 29/351 [01:39<18:26,  3.44s/it]\r  9%|▊         | 30/351 [01:43<18:35,  3.48s/it]\r  9%|▉         | 31/351 [01:46<18:24,  3.45s/it]\r  9%|▉         | 32/351 [01:49<18:15,  3.43s/it]\r  9%|▉         | 33/351 [01:53<18:13,  3.44s/it]\r 10%|▉         | 34/351 [01:56<18:06,  3.43s/it]\r 10%|▉         | 35/351 [02:00<18:04,  3.43s/it]\r 10%|█         | 36/351 [02:03<17:58,  3.42s/it]\r 11%|█         | 37/351 [02:06<17:52,  3.42s/it]\r 11%|█         | 38/351 [02:10<17:49,  3.42s/it]\r 11%|█         | 39/351 [02:13<17:44,  3.41s/it]\r 11%|█▏        | 40/351 [02:17<17:43,  3.42s/it]\r 12%|█▏        | 41/351 [02:20<17:43,  3.43s/it]\r 12%|█▏        | 42/351 [02:24<17:36,  3.42s/it]\r 12%|█▏        | 43/351 [02:27<17:33,  3.42s/it]\r 13%|█▎        | 44/351 [02:30<17:36,  3.44s/it]\r 13%|█▎        | 45/351 [02:34<17:29,  3.43s/it]\r 13%|█▎        | 46/351 [02:37<17:22,  3.42s/it]\r 13%|█▎        | 47/351 [02:41<17:19,  3.42s/it]\r 14%|█▎        | 48/351 [02:44<17:13,  3.41s/it]\r 14%|█▍        | 49/351 [02:48<17:15,  3.43s/it]\r 14%|█▍        | 50/351 [02:51<17:09,  3.42s/it]\r 15%|█▍        | 51/351 [02:54<17:06,  3.42s/it]\r 15%|█▍        | 52/351 [02:58<17:10,  3.45s/it]\r 15%|█▌        | 53/351 [03:01<17:02,  3.43s/it]\r 15%|█▌        | 54/351 [03:05<16:57,  3.43s/it]\r 16%|█▌        | 55/351 [03:08<16:51,  3.42s/it]\r 16%|█▌        | 56/351 [03:12<16:49,  3.42s/it]\r 16%|█▌        | 57/351 [03:15<16:49,  3.43s/it]\r 17%|█▋        | 58/351 [03:18<16:42,  3.42s/it]\r 17%|█▋        | 59/351 [03:22<16:37,  3.41s/it]\r 17%|█▋        | 60/351 [03:25<16:32,  3.41s/it]\r 17%|█▋        | 61/351 [03:29<16:29,  3.41s/it]\r 18%|█▊        | 62/351 [03:32<16:29,  3.42s/it]\r 18%|█▊        | 63/351 [03:35<16:23,  3.42s/it]\r 18%|█▊        | 64/351 [03:39<16:21,  3.42s/it]\r 19%|█▊        | 65/351 [03:42<16:21,  3.43s/it]\r 19%|█▉        | 66/351 [03:46<16:14,  3.42s/it]\r 19%|█▉        | 67/351 [03:49<16:09,  3.42s/it]\r 19%|█▉        | 68/351 [03:53<16:10,  3.43s/it]\r 20%|█▉        | 69/351 [03:56<16:04,  3.42s/it]\r 20%|█▉        | 70/351 [03:59<15:59,  3.41s/it]\r 20%|██        | 71/351 [04:03<15:55,  3.41s/it]\r 21%|██        | 72/351 [04:06<15:50,  3.41s/it]\r 21%|██        | 73/351 [04:10<15:47,  3.41s/it]\r 21%|██        | 74/351 [04:13<15:44,  3.41s/it]\r 21%|██▏       | 75/351 [04:16<15:39,  3.41s/it]\r 22%|██▏       | 76/351 [04:20<15:35,  3.40s/it]\r 22%|██▏       | 77/351 [04:23<15:31,  3.40s/it]\r 22%|██▏       | 78/351 [04:27<15:28,  3.40s/it]\r 23%|██▎       | 79/351 [04:30<15:24,  3.40s/it]\r 23%|██▎       | 80/351 [04:33<15:20,  3.40s/it]\r 23%|██▎       | 81/351 [04:37<15:17,  3.40s/it]\r 23%|██▎       | 82/351 [04:40<15:14,  3.40s/it]\r 24%|██▎       | 83/351 [04:44<15:12,  3.41s/it]\r 24%|██▍       | 84/351 [04:47<15:08,  3.40s/it]\r 24%|██▍       | 85/351 [04:50<15:05,  3.40s/it]\r 25%|██▍       | 86/351 [04:54<15:00,  3.40s/it]\r 25%|██▍       | 87/351 [04:57<14:57,  3.40s/it]\r 25%|██▌       | 88/351 [05:01<14:54,  3.40s/it]\r 25%|██▌       | 89/351 [05:04<14:50,  3.40s/it]\r 26%|██▌       | 90/351 [05:07<14:47,  3.40s/it]\r 26%|██▌       | 91/351 [05:11<14:43,  3.40s/it]\r 26%|██▌       | 92/351 [05:14<14:40,  3.40s/it]\r 26%|██▋       | 93/351 [05:18<14:36,  3.40s/it]\r 27%|██▋       | 94/351 [05:21<14:40,  3.43s/it]\r 27%|██▋       | 95/351 [05:25<14:40,  3.44s/it]\r 27%|██▋       | 96/351 [05:28<14:34,  3.43s/it]\r 28%|██▊       | 97/351 [05:31<14:28,  3.42s/it]\r 28%|██▊       | 98/351 [05:35<14:23,  3.41s/it]\r 28%|██▊       | 99/351 [05:38<14:18,  3.41s/it]\r 28%|██▊       | 100/351 [05:42<14:17,  3.41s/it]\r 29%|██▉       | 101/351 [05:45<14:12,  3.41s/it]\r 29%|██▉       | 102/351 [05:48<14:08,  3.41s/it]\r 29%|██▉       | 103/351 [05:52<14:04,  3.40s/it]\r 30%|██▉       | 104/351 [05:55<14:00,  3.40s/it]\r 30%|██▉       | 105/351 [05:59<13:56,  3.40s/it]\r 30%|███       | 106/351 [06:02<13:55,  3.41s/it]\r 30%|███       | 107/351 [06:05<13:51,  3.41s/it]\r 31%|███       | 108/351 [06:09<13:47,  3.41s/it]\r 31%|███       | 109/351 [06:12<13:43,  3.40s/it]\r 31%|███▏      | 110/351 [06:16<13:44,  3.42s/it]\r 32%|███▏      | 111/351 [06:19<13:39,  3.41s/it]\r 32%|███▏      | 112/351 [06:22<13:34,  3.41s/it]\r 32%|███▏      | 113/351 [06:26<13:34,  3.42s/it]\r 32%|███▏      | 114/351 [06:29<13:29,  3.42s/it]\r 33%|███▎      | 115/351 [06:33<13:24,  3.41s/it]\r 33%|███▎      | 116/351 [06:36<13:20,  3.41s/it]\r 33%|███▎      | 117/351 [06:40<13:16,  3.40s/it]\r 34%|███▎      | 118/351 [06:43<13:12,  3.40s/it]\r 34%|███▍      | 119/351 [06:46<13:08,  3.40s/it]\r 34%|███▍      | 120/351 [06:50<13:09,  3.42s/it]\r 34%|███▍      | 121/351 [06:53<13:07,  3.42s/it]\r 35%|███▍      | 122/351 [06:57<13:06,  3.44s/it]\r 35%|███▌      | 123/351 [07:00<13:00,  3.42s/it]\r 35%|███▌      | 124/351 [07:03<12:55,  3.42s/it]\r 36%|███▌      | 125/351 [07:07<12:57,  3.44s/it]\r 36%|███▌      | 126/351 [07:10<12:50,  3.43s/it]\r 36%|███▌      | 127/351 [07:14<12:45,  3.42s/it]\r 36%|███▋      | 128/351 [07:17<12:44,  3.43s/it]\r 37%|███▋      | 129/351 [07:21<12:38,  3.42s/it]\r 37%|███▋      | 130/351 [07:24<12:34,  3.41s/it]\r 37%|███▋      | 131/351 [07:27<12:36,  3.44s/it]\r 38%|███▊      | 132/351 [07:31<12:29,  3.42s/it]\r 38%|███▊      | 133/351 [07:34<12:24,  3.42s/it]\r 38%|███▊      | 134/351 [07:38<12:20,  3.41s/it]\r 38%|███▊      | 135/351 [07:41<12:16,  3.41s/it]\r 39%|███▊      | 136/351 [07:45<12:16,  3.43s/it]\r 39%|███▉      | 137/351 [07:48<12:11,  3.42s/it]\r 39%|███▉      | 138/351 [07:51<12:10,  3.43s/it]\r 40%|███▉      | 139/351 [07:55<12:04,  3.42s/it]\r 40%|███▉      | 140/351 [07:58<11:59,  3.41s/it]\r 40%|████      | 141/351 [08:02<11:57,  3.42s/it]\r 40%|████      | 142/351 [08:05<11:53,  3.41s/it]\r 41%|████      | 143/351 [08:08<11:48,  3.41s/it]\r 41%|████      | 144/351 [08:12<11:44,  3.40s/it]\r 41%|████▏     | 145/351 [08:15<11:46,  3.43s/it]\r 42%|████▏     | 146/351 [08:19<11:40,  3.42s/it]\r 42%|████▏     | 147/351 [08:22<11:36,  3.41s/it]\r 42%|████▏     | 148/351 [08:26<11:35,  3.42s/it]\r 42%|████▏     | 149/351 [08:29<11:30,  3.42s/it]\r 43%|████▎     | 150/351 [08:32<11:25,  3.41s/it]\r 43%|████▎     | 151/351 [08:36<11:30,  3.45s/it]\r 43%|████▎     | 152/351 [08:39<11:23,  3.44s/it]\r 44%|████▎     | 153/351 [08:43<11:18,  3.42s/it]\r 44%|████▍     | 154/351 [08:46<11:14,  3.42s/it]\r 44%|████▍     | 155/351 [08:50<11:09,  3.42s/it]\r 44%|████▍     | 156/351 [08:53<11:04,  3.41s/it]\r 45%|████▍     | 157/351 [08:56<11:00,  3.41s/it]\r 45%|████▌     | 158/351 [09:00<10:56,  3.40s/it]\r 45%|████▌     | 159/351 [09:03<10:53,  3.40s/it]\r 46%|████▌     | 160/351 [09:07<10:51,  3.41s/it]\r 46%|████▌     | 161/351 [09:10<10:47,  3.41s/it]\r 46%|████▌     | 162/351 [09:13<10:43,  3.41s/it]\r 46%|████▋     | 163/351 [09:17<10:44,  3.43s/it]\r 47%|████▋     | 164/351 [09:20<10:39,  3.42s/it]\r 47%|████▋     | 165/351 [09:24<10:34,  3.41s/it]\r 47%|████▋     | 166/351 [09:27<10:36,  3.44s/it]\r 48%|████▊     | 167/351 [09:31<10:31,  3.43s/it]\r 48%|████▊     | 168/351 [09:34<10:26,  3.42s/it]\r 48%|████▊     | 169/351 [09:37<10:25,  3.43s/it]\r 48%|████▊     | 170/351 [09:41<10:19,  3.42s/it]\r 49%|████▊     | 171/351 [09:44<10:14,  3.41s/it]\r 49%|████▉     | 172/351 [09:48<10:10,  3.41s/it]\r 49%|████▉     | 173/351 [09:51<10:06,  3.41s/it]\r 50%|████▉     | 174/351 [09:54<10:04,  3.41s/it]\r 50%|████▉     | 175/351 [09:58<09:59,  3.41s/it]\r 50%|█████     | 176/351 [10:01<09:56,  3.41s/it]\r 50%|█████     | 177/351 [10:05<09:52,  3.40s/it]\r 51%|█████     | 178/351 [10:08<09:48,  3.40s/it]\r 51%|█████     | 179/351 [10:11<09:44,  3.40s/it]\r 51%|█████▏    | 180/351 [10:15<09:41,  3.40s/it]\r 52%|█████▏    | 181/351 [10:18<09:37,  3.40s/it]\r 52%|█████▏    | 182/351 [10:22<09:34,  3.40s/it]\r 52%|█████▏    | 183/351 [10:25<09:32,  3.41s/it]\r 52%|█████▏    | 184/351 [10:28<09:30,  3.41s/it]\r 53%|█████▎    | 185/351 [10:32<09:27,  3.42s/it]\r 53%|█████▎    | 186/351 [10:35<09:22,  3.41s/it]\r 53%|█████▎    | 187/351 [10:39<09:23,  3.44s/it]\r 54%|█████▎    | 188/351 [10:42<09:18,  3.42s/it]\r 54%|█████▍    | 189/351 [10:46<09:13,  3.42s/it]\r 54%|█████▍    | 190/351 [10:49<09:11,  3.42s/it]\r 54%|█████▍    | 191/351 [10:52<09:06,  3.42s/it]\r 55%|█████▍    | 192/351 [10:56<09:07,  3.45s/it]\r 55%|█████▍    | 193/351 [10:59<09:04,  3.45s/it]\r 55%|█████▌    | 194/351 [11:03<08:58,  3.43s/it]\r 56%|█████▌    | 195/351 [11:06<08:55,  3.44s/it]\r 56%|█████▌    | 196/351 [11:10<08:54,  3.45s/it]\r 56%|█████▌    | 197/351 [11:13<08:48,  3.43s/it]\r 56%|█████▋    | 198/351 [11:17<08:47,  3.45s/it]\r 57%|█████▋    | 199/351 [11:20<08:41,  3.43s/it]\r 57%|█████▋    | 200/351 [11:23<08:36,  3.42s/it]\r 57%|█████▋    | 201/351 [11:27<08:34,  3.43s/it]\r 58%|█████▊    | 202/351 [11:30<08:30,  3.43s/it]\r 58%|█████▊    | 203/351 [11:34<08:25,  3.42s/it]\r 58%|█████▊    | 204/351 [11:37<08:21,  3.41s/it]\r 58%|█████▊    | 205/351 [11:41<08:24,  3.45s/it]\r 59%|█████▊    | 206/351 [11:44<08:18,  3.44s/it]\r 59%|█████▉    | 207/351 [11:47<08:13,  3.43s/it]\r 59%|█████▉    | 208/351 [11:51<08:08,  3.42s/it]\r 60%|█████▉    | 209/351 [11:54<08:06,  3.43s/it]\r 60%|█████▉    | 210/351 [11:58<08:03,  3.43s/it]\r 60%|██████    | 211/351 [12:01<07:58,  3.42s/it]\r 60%|██████    | 212/351 [12:04<07:55,  3.42s/it]\r 61%|██████    | 213/351 [12:08<07:51,  3.41s/it]\r 61%|██████    | 214/351 [12:11<07:47,  3.41s/it]\r 61%|██████▏   | 215/351 [12:15<07:43,  3.41s/it]\r 62%|██████▏   | 216/351 [12:18<07:39,  3.40s/it]\r 62%|██████▏   | 217/351 [12:21<07:35,  3.40s/it]\r 62%|██████▏   | 218/351 [12:25<07:32,  3.40s/it]\r 62%|██████▏   | 219/351 [12:28<07:29,  3.40s/it]\r 63%|██████▎   | 220/351 [12:32<07:25,  3.40s/it]\r 63%|██████▎   | 221/351 [12:35<07:23,  3.41s/it]\r 63%|██████▎   | 222/351 [12:38<07:19,  3.41s/it]\r 64%|██████▎   | 223/351 [12:42<07:15,  3.40s/it]\r 64%|██████▍   | 224/351 [12:45<07:11,  3.40s/it]\r 64%|██████▍   | 225/351 [12:49<07:08,  3.40s/it]\r 64%|██████▍   | 226/351 [12:52<07:05,  3.40s/it]\r 65%|██████▍   | 227/351 [12:55<07:01,  3.40s/it]\r 65%|██████▍   | 228/351 [12:59<06:59,  3.41s/it]\r 65%|██████▌   | 229/351 [13:02<06:55,  3.41s/it]\r 66%|██████▌   | 230/351 [13:06<06:51,  3.40s/it]\r 66%|██████▌   | 231/351 [13:09<06:49,  3.41s/it]\r 66%|██████▌   | 232/351 [13:13<06:45,  3.41s/it]\r 66%|██████▋   | 233/351 [13:16<06:41,  3.40s/it]\r 67%|██████▋   | 234/351 [13:19<06:39,  3.41s/it]\r 67%|██████▋   | 235/351 [13:23<06:35,  3.41s/it]\r 67%|██████▋   | 236/351 [13:26<06:31,  3.40s/it]\r 68%|██████▊   | 237/351 [13:30<06:29,  3.42s/it]\r 68%|██████▊   | 238/351 [13:33<06:25,  3.41s/it]\r 68%|██████▊   | 239/351 [13:36<06:21,  3.41s/it]\r 68%|██████▊   | 240/351 [13:40<06:19,  3.42s/it]\r 69%|██████▊   | 241/351 [13:43<06:15,  3.42s/it]\r 69%|██████▉   | 242/351 [13:47<06:11,  3.41s/it]\r 69%|██████▉   | 243/351 [13:50<06:07,  3.41s/it]\r 70%|██████▉   | 244/351 [13:54<06:08,  3.45s/it]\r 70%|██████▉   | 245/351 [13:57<06:03,  3.43s/it]\r 70%|███████   | 246/351 [14:00<05:59,  3.42s/it]\r 70%|███████   | 247/351 [14:04<05:54,  3.41s/it]\r 71%|███████   | 248/351 [14:07<05:51,  3.41s/it]\r 71%|███████   | 249/351 [14:11<05:50,  3.44s/it]\r 71%|███████   | 250/351 [14:14<05:45,  3.42s/it]\r 72%|███████▏  | 251/351 [14:17<05:41,  3.41s/it]\r 72%|███████▏  | 252/351 [14:21<05:37,  3.41s/it]\r 72%|███████▏  | 253/351 [14:24<05:33,  3.40s/it]\r 72%|███████▏  | 254/351 [14:28<05:29,  3.40s/it]\r 73%|███████▎  | 255/351 [14:31<05:26,  3.40s/it]\r 73%|███████▎  | 256/351 [14:34<05:22,  3.40s/it]\r 73%|███████▎  | 257/351 [14:38<05:19,  3.40s/it]\r 74%|███████▎  | 258/351 [14:41<05:16,  3.40s/it]\r 74%|███████▍  | 259/351 [14:45<05:12,  3.40s/it]\r 74%|███████▍  | 260/351 [14:48<05:09,  3.40s/it]\r 74%|███████▍  | 261/351 [14:51<05:05,  3.40s/it]\r 75%|███████▍  | 262/351 [14:55<05:02,  3.40s/it]\r 75%|███████▍  | 263/351 [14:58<05:00,  3.41s/it]\r 75%|███████▌  | 264/351 [15:02<04:56,  3.41s/it]\r 75%|███████▌  | 265/351 [15:05<04:56,  3.45s/it]\r 76%|███████▌  | 266/351 [15:09<04:51,  3.43s/it]\r 76%|███████▌  | 267/351 [15:12<04:47,  3.42s/it]\r 76%|███████▋  | 268/351 [15:15<04:43,  3.41s/it]\r 77%|███████▋  | 269/351 [15:19<04:39,  3.41s/it]\r 77%|███████▋  | 270/351 [15:22<04:36,  3.41s/it]\r 77%|███████▋  | 271/351 [15:26<04:34,  3.43s/it]\r 77%|███████▋  | 272/351 [15:29<04:30,  3.42s/it]\r 78%|███████▊  | 273/351 [15:32<04:26,  3.41s/it]\r 78%|███████▊  | 274/351 [15:36<04:23,  3.42s/it]\r 78%|███████▊  | 275/351 [15:39<04:19,  3.41s/it]\r 79%|███████▊  | 276/351 [15:43<04:15,  3.41s/it]\r 79%|███████▉  | 277/351 [15:46<04:15,  3.45s/it]\r 79%|███████▉  | 278/351 [15:50<04:10,  3.43s/it]\r 79%|███████▉  | 279/351 [15:53<04:06,  3.42s/it]\r 80%|███████▉  | 280/351 [15:56<04:03,  3.43s/it]\r 80%|████████  | 281/351 [16:00<03:59,  3.42s/it]\r 80%|████████  | 282/351 [16:03<03:55,  3.41s/it]\r 81%|████████  | 283/351 [16:07<03:53,  3.43s/it]\r 81%|████████  | 284/351 [16:10<03:49,  3.42s/it]\r 81%|████████  | 285/351 [16:14<03:45,  3.42s/it]\r 81%|████████▏ | 286/351 [16:17<03:41,  3.41s/it]\r 82%|████████▏ | 287/351 [16:20<03:38,  3.41s/it]\r 82%|████████▏ | 288/351 [16:24<03:36,  3.44s/it]\r 82%|████████▏ | 289/351 [16:27<03:34,  3.47s/it]\r 83%|████████▎ | 290/351 [16:31<03:30,  3.45s/it]\r 83%|████████▎ | 291/351 [16:34<03:26,  3.44s/it]\r 83%|████████▎ | 292/351 [16:38<03:22,  3.43s/it]\r 83%|████████▎ | 293/351 [16:41<03:18,  3.42s/it]\r 84%|████████▍ | 294/351 [16:44<03:14,  3.42s/it]\r 84%|████████▍ | 295/351 [16:48<03:11,  3.41s/it]\r 84%|████████▍ | 296/351 [16:51<03:07,  3.41s/it]\r 85%|████████▍ | 297/351 [16:55<03:05,  3.44s/it]\r 85%|████████▍ | 298/351 [16:58<03:01,  3.42s/it]\r 85%|████████▌ | 299/351 [17:02<02:58,  3.43s/it]\r 85%|████████▌ | 300/351 [17:05<02:54,  3.42s/it]\r 86%|████████▌ | 301/351 [17:08<02:50,  3.41s/it]\r 86%|████████▌ | 302/351 [17:12<02:47,  3.42s/it]\r 86%|████████▋ | 303/351 [17:15<02:45,  3.44s/it]\r 87%|████████▋ | 304/351 [17:19<02:41,  3.43s/it]\r 87%|████████▋ | 305/351 [17:22<02:37,  3.42s/it]\r 87%|████████▋ | 306/351 [17:26<02:33,  3.42s/it]\r 87%|████████▋ | 307/351 [17:29<02:30,  3.41s/it]\r 88%|████████▊ | 308/351 [17:32<02:26,  3.41s/it]\r 88%|████████▊ | 309/351 [17:36<02:23,  3.41s/it]\r 88%|████████▊ | 310/351 [17:39<02:19,  3.40s/it]\r 89%|████████▊ | 311/351 [17:43<02:16,  3.40s/it]\r 89%|████████▉ | 312/351 [17:46<02:12,  3.40s/it]\r 89%|████████▉ | 313/351 [17:49<02:09,  3.40s/it]\r 89%|████████▉ | 314/351 [17:53<02:05,  3.40s/it]\r 90%|████████▉ | 315/351 [17:56<02:02,  3.40s/it]\r 90%|█████████ | 316/351 [18:00<01:58,  3.40s/it]\r 90%|█████████ | 317/351 [18:03<01:55,  3.40s/it]\r 91%|█████████ | 318/351 [18:06<01:52,  3.40s/it]\r 91%|█████████ | 319/351 [18:10<01:48,  3.40s/it]\r 91%|█████████ | 320/351 [18:13<01:45,  3.41s/it]\r 91%|█████████▏| 321/351 [18:17<01:42,  3.43s/it]\r 92%|█████████▏| 322/351 [18:20<01:39,  3.42s/it]\r 92%|█████████▏| 323/351 [18:23<01:36,  3.43s/it]\r 92%|█████████▏| 324/351 [18:27<01:33,  3.46s/it]\r 93%|█████████▎| 325/351 [18:30<01:29,  3.44s/it]\r 93%|█████████▎| 326/351 [18:34<01:25,  3.43s/it]\r 93%|█████████▎| 327/351 [18:37<01:22,  3.42s/it]\r 93%|█████████▎| 328/351 [18:41<01:18,  3.41s/it]\r 94%|█████████▎| 329/351 [18:44<01:15,  3.41s/it]\r 94%|█████████▍| 330/351 [18:47<01:11,  3.41s/it]\r 94%|█████████▍| 331/351 [18:51<01:08,  3.40s/it]\r 95%|█████████▍| 332/351 [18:54<01:04,  3.40s/it]\r 95%|█████████▍| 333/351 [18:58<01:01,  3.42s/it]\r 95%|█████████▌| 334/351 [19:01<00:58,  3.43s/it]\r 95%|█████████▌| 335/351 [19:05<00:54,  3.42s/it]\r 96%|█████████▌| 336/351 [19:08<00:51,  3.41s/it]\r 96%|█████████▌| 337/351 [19:11<00:47,  3.42s/it]\r 96%|█████████▋| 338/351 [19:15<00:44,  3.42s/it]\r 97%|█████████▋| 339/351 [19:18<00:40,  3.41s/it]\r 97%|█████████▋| 340/351 [19:22<00:37,  3.41s/it]\r 97%|█████████▋| 341/351 [19:25<00:34,  3.42s/it]\r 97%|█████████▋| 342/351 [19:28<00:30,  3.41s/it]\r 98%|█████████▊| 343/351 [19:32<00:27,  3.41s/it]\r 98%|█████████▊| 344/351 [19:35<00:23,  3.41s/it]\r 98%|█████████▊| 345/351 [19:39<00:20,  3.41s/it]\r 99%|█████████▊| 346/351 [19:42<00:17,  3.40s/it]\r 99%|█████████▉| 347/351 [19:45<00:13,  3.42s/it]\r 99%|█████████▉| 348/351 [19:49<00:10,  3.42s/it]\r 99%|█████████▉| 349/351 [19:52<00:06,  3.41s/it]\r100%|█████████▉| 350/351 [19:56<00:03,  3.41s/it]\r100%|██████████| 351/351 [19:56<00:00,  2.54s/it]\r100%|██████████| 351/351 [19:56<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING POS ACCURACY: 0.9562676213308923\n",
      "count_correct_p 65291\n",
      "count_active_tokens_p 78673\n",
      "precision: 0.8299035247162305\n",
      "count_active_tokens_r 84556\n",
      "count_correct_r 65291\n",
      "recall: 0.7721628270022234\n",
      "F1: 0\n",
      "TRAINING FEAT ACCURACY: 0.9615882636193639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "\r  1%|          | 1/84 [00:01<01:48,  1.31s/it]\r  2%|▏         | 2/84 [00:02<01:41,  1.23s/it]\r  4%|▎         | 3/84 [00:03<01:38,  1.21s/it]\r  5%|▍         | 4/84 [00:04<01:36,  1.20s/it]\r  6%|▌         | 5/84 [00:06<01:34,  1.20s/it]\r  7%|▋         | 6/84 [00:07<01:33,  1.19s/it]\r  8%|▊         | 7/84 [00:08<01:31,  1.19s/it]\r 10%|▉         | 8/84 [00:09<01:30,  1.19s/it]\r 11%|█         | 9/84 [00:10<01:29,  1.19s/it]\r 12%|█▏        | 10/84 [00:11<01:28,  1.19s/it]\r 13%|█▎        | 11/84 [00:13<01:26,  1.19s/it]\r 14%|█▍        | 12/84 [00:14<01:25,  1.19s/it]\r 15%|█▌        | 13/84 [00:15<01:24,  1.19s/it]\r 17%|█▋        | 14/84 [00:16<01:23,  1.19s/it]\r 18%|█▊        | 15/84 [00:17<01:21,  1.19s/it]\r 19%|█▉        | 16/84 [00:19<01:20,  1.19s/it]\r 20%|██        | 17/84 [00:20<01:19,  1.19s/it]\r 21%|██▏       | 18/84 [00:21<01:18,  1.19s/it]\r 23%|██▎       | 19/84 [00:22<01:17,  1.19s/it]\r 24%|██▍       | 20/84 [00:23<01:16,  1.19s/it]\r 25%|██▌       | 21/84 [00:25<01:14,  1.19s/it]\r 26%|██▌       | 22/84 [00:26<01:13,  1.19s/it]\r 27%|██▋       | 23/84 [00:27<01:12,  1.19s/it]\r 29%|██▊       | 24/84 [00:28<01:11,  1.19s/it]\r 30%|██▉       | 25/84 [00:29<01:10,  1.19s/it]\r 31%|███       | 26/84 [00:30<01:08,  1.19s/it]\r 32%|███▏      | 27/84 [00:32<01:07,  1.19s/it]\r 33%|███▎      | 28/84 [00:33<01:06,  1.18s/it]\r 35%|███▍      | 29/84 [00:34<01:05,  1.19s/it]\r 36%|███▌      | 30/84 [00:35<01:04,  1.19s/it]\r 37%|███▋      | 31/84 [00:36<01:02,  1.19s/it]\r 38%|███▊      | 32/84 [00:38<01:01,  1.19s/it]\r 39%|███▉      | 33/84 [00:39<01:00,  1.19s/it]\r 40%|████      | 34/84 [00:40<00:59,  1.19s/it]\r 42%|████▏     | 35/84 [00:41<00:58,  1.19s/it]\r 43%|████▎     | 36/84 [00:42<00:56,  1.19s/it]\r 44%|████▍     | 37/84 [00:44<00:55,  1.19s/it]\r 45%|████▌     | 38/84 [00:45<00:54,  1.19s/it]\r 46%|████▋     | 39/84 [00:46<00:53,  1.19s/it]\r 48%|████▊     | 40/84 [00:47<00:52,  1.19s/it]\r 49%|████▉     | 41/84 [00:48<00:50,  1.19s/it]\r 50%|█████     | 42/84 [00:49<00:49,  1.18s/it]\r 51%|█████     | 43/84 [00:51<00:48,  1.19s/it]\r 52%|█████▏    | 44/84 [00:52<00:47,  1.19s/it]\r 54%|█████▎    | 45/84 [00:53<00:46,  1.19s/it]\r 55%|█████▍    | 46/84 [00:54<00:45,  1.19s/it]\r 56%|█████▌    | 47/84 [00:55<00:43,  1.19s/it]\r 57%|█████▋    | 48/84 [00:57<00:42,  1.18s/it]\r 58%|█████▊    | 49/84 [00:58<00:41,  1.20s/it]\r 60%|█████▉    | 50/84 [00:59<00:40,  1.19s/it]\r 61%|██████    | 51/84 [01:00<00:39,  1.19s/it]\r 62%|██████▏   | 52/84 [01:01<00:38,  1.19s/it]\r 63%|██████▎   | 53/84 [01:03<00:36,  1.19s/it]\r 64%|██████▍   | 54/84 [01:04<00:35,  1.19s/it]\r 65%|██████▌   | 55/84 [01:05<00:34,  1.18s/it]\r 67%|██████▋   | 56/84 [01:06<00:33,  1.19s/it]\r 68%|██████▊   | 57/84 [01:07<00:32,  1.19s/it]\r 69%|██████▉   | 58/84 [01:09<00:30,  1.19s/it]\r 70%|███████   | 59/84 [01:10<00:29,  1.19s/it]\r 71%|███████▏  | 60/84 [01:11<00:28,  1.19s/it]\r 73%|███████▎  | 61/84 [01:12<00:27,  1.18s/it]\r 74%|███████▍  | 62/84 [01:13<00:26,  1.18s/it]\r 75%|███████▌  | 63/84 [01:14<00:24,  1.18s/it]\r 76%|███████▌  | 64/84 [01:16<00:23,  1.19s/it]\r 77%|███████▋  | 65/84 [01:17<00:22,  1.19s/it]\r 79%|███████▊  | 66/84 [01:18<00:21,  1.19s/it]\r 80%|███████▉  | 67/84 [01:19<00:20,  1.19s/it]\r 81%|████████  | 68/84 [01:20<00:19,  1.19s/it]\r 82%|████████▏ | 69/84 [01:22<00:17,  1.19s/it]\r 83%|████████▎ | 70/84 [01:23<00:16,  1.19s/it]\r 85%|████████▍ | 71/84 [01:24<00:15,  1.19s/it]\r 86%|████████▌ | 72/84 [01:25<00:14,  1.18s/it]\r 87%|████████▋ | 73/84 [01:26<00:13,  1.19s/it]\r 88%|████████▊ | 74/84 [01:27<00:11,  1.19s/it]\r 89%|████████▉ | 75/84 [01:29<00:10,  1.19s/it]\r 90%|█████████ | 76/84 [01:30<00:09,  1.19s/it]\r 92%|█████████▏| 77/84 [01:31<00:08,  1.19s/it]\r 93%|█████████▎| 78/84 [01:32<00:07,  1.19s/it]\r 94%|█████████▍| 79/84 [01:33<00:05,  1.19s/it]\r 95%|█████████▌| 80/84 [01:35<00:04,  1.19s/it]\r 96%|█████████▋| 81/84 [01:36<00:03,  1.20s/it]\r 98%|█████████▊| 82/84 [01:37<00:02,  1.19s/it]\r 99%|█████████▉| 83/84 [01:38<00:01,  1.20s/it]\r100%|██████████| 84/84 [01:39<00:00,  1.04it/s]\r100%|██████████| 84/84 [01:39<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION POS ACCURACY: 0.9631947280618567\n",
      "count_correct_p 21151\n",
      "count_active_tokens_p 24297\n",
      "precision: 0.8705189941144997\n",
      "count_active_tokens_r 25013\n",
      "count_correct_r 21151\n",
      "recall: 0.8456002878503178\n",
      "F1: 0\n",
      "VALIDATION FEAT ACCURACY: 0.9660397472017056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/351 [00:00<?, ?it/s]/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/aailab_conda/anaconda3/envs/kbvqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "\r  0%|          | 1/351 [00:03<20:27,  3.51s/it]\r  1%|          | 2/351 [00:06<20:01,  3.44s/it]\r  1%|          | 3/351 [00:10<19:54,  3.43s/it]\r  1%|          | 4/351 [00:13<19:50,  3.43s/it]\r  1%|▏         | 5/351 [00:17<19:44,  3.42s/it]\r  2%|▏         | 6/351 [00:20<19:39,  3.42s/it]\r  2%|▏         | 7/351 [00:23<19:35,  3.42s/it]\r  2%|▏         | 8/351 [00:27<19:30,  3.41s/it]\r  3%|▎         | 9/351 [00:30<19:26,  3.41s/it]\r  3%|▎         | 10/351 [00:34<19:22,  3.41s/it]\r  3%|▎         | 11/351 [00:37<19:18,  3.41s/it]\r  3%|▎         | 12/351 [00:41<19:14,  3.41s/it]\r  4%|▎         | 13/351 [00:44<19:10,  3.40s/it]\r  4%|▍         | 14/351 [00:47<19:07,  3.41s/it]\r  4%|▍         | 15/351 [00:51<19:03,  3.40s/it]\r  5%|▍         | 16/351 [00:54<19:06,  3.42s/it]\r  5%|▍         | 17/351 [00:58<19:01,  3.42s/it]\r  5%|▌         | 18/351 [01:01<18:55,  3.41s/it]\r  5%|▌         | 19/351 [01:04<19:00,  3.44s/it]\r  6%|▌         | 20/351 [01:08<18:53,  3.43s/it]\r  6%|▌         | 21/351 [01:11<18:50,  3.42s/it]\r  6%|▋         | 22/351 [01:15<18:50,  3.44s/it]\r  7%|▋         | 23/351 [01:18<18:43,  3.42s/it]\r  7%|▋         | 24/351 [01:22<18:37,  3.42s/it]\r  7%|▋         | 25/351 [01:25<18:46,  3.46s/it]\r  7%|▋         | 26/351 [01:29<18:37,  3.44s/it]\r  8%|▊         | 27/351 [01:32<18:36,  3.44s/it]\r  8%|▊         | 28/351 [01:35<18:28,  3.43s/it]\r  8%|▊         | 29/351 [01:39<18:24,  3.43s/it]\r  9%|▊         | 30/351 [01:42<18:17,  3.42s/it]\r  9%|▉         | 31/351 [01:46<18:12,  3.41s/it]\r  9%|▉         | 32/351 [01:49<18:07,  3.41s/it]\r  9%|▉         | 33/351 [01:52<18:02,  3.41s/it]\r 10%|▉         | 34/351 [01:56<17:58,  3.40s/it]\r 10%|▉         | 35/351 [01:59<17:56,  3.41s/it]\r 10%|█         | 36/351 [02:03<17:52,  3.40s/it]\r 11%|█         | 37/351 [02:06<17:48,  3.40s/it]\r 11%|█         | 38/351 [02:09<17:47,  3.41s/it]\r 11%|█         | 39/351 [02:13<17:47,  3.42s/it]\r 11%|█▏        | 40/351 [02:16<17:43,  3.42s/it]\r 12%|█▏        | 41/351 [02:20<17:38,  3.41s/it]\r 12%|█▏        | 42/351 [02:23<17:33,  3.41s/it]\r 12%|█▏        | 43/351 [02:27<17:38,  3.44s/it]\r 13%|█▎        | 44/351 [02:30<17:31,  3.43s/it]\r 13%|█▎        | 45/351 [02:33<17:25,  3.42s/it]\r 13%|█▎        | 46/351 [02:37<17:20,  3.41s/it]\r 13%|█▎        | 47/351 [02:40<17:16,  3.41s/it]\r 14%|█▎        | 48/351 [02:44<17:11,  3.41s/it]\r 14%|█▍        | 49/351 [02:47<17:11,  3.41s/it]\r 14%|█▍        | 50/351 [02:50<17:06,  3.41s/it]\r 15%|█▍        | 51/351 [02:54<17:02,  3.41s/it]\r 15%|█▍        | 52/351 [02:57<16:58,  3.40s/it]\r 15%|█▌        | 53/351 [03:01<16:54,  3.40s/it]\r 15%|█▌        | 54/351 [03:04<16:50,  3.40s/it]\r 16%|█▌        | 55/351 [03:07<16:47,  3.40s/it]\r 16%|█▌        | 56/351 [03:11<16:50,  3.42s/it]\r 16%|█▌        | 57/351 [03:14<16:44,  3.42s/it]\r 17%|█▋        | 58/351 [03:18<16:41,  3.42s/it]\r 17%|█▋        | 59/351 [03:21<16:36,  3.41s/it]\r 17%|█▋        | 60/351 [03:25<16:32,  3.41s/it]\r 17%|█▋        | 61/351 [03:28<16:27,  3.41s/it]\r 18%|█▊        | 62/351 [03:31<16:24,  3.41s/it]\r 18%|█▊        | 63/351 [03:35<16:28,  3.43s/it]\r 18%|█▊        | 64/351 [03:38<16:23,  3.43s/it]\r 19%|█▊        | 65/351 [03:42<16:17,  3.42s/it]\r 19%|█▉        | 66/351 [03:45<16:12,  3.41s/it]\r 19%|█▉        | 67/351 [03:48<16:07,  3.41s/it]\r 19%|█▉        | 68/351 [03:52<16:03,  3.40s/it]\r 20%|█▉        | 69/351 [03:55<16:07,  3.43s/it]\r 20%|█▉        | 70/351 [03:59<16:00,  3.42s/it]\r 20%|██        | 71/351 [04:02<16:04,  3.44s/it]\r 21%|██        | 72/351 [04:06<15:56,  3.43s/it]\r 21%|██        | 73/351 [04:09<15:52,  3.42s/it]\r 21%|██        | 74/351 [04:12<15:46,  3.42s/it]\r 21%|██▏       | 75/351 [04:16<15:41,  3.41s/it]\r 22%|██▏       | 76/351 [04:19<15:38,  3.41s/it]\r 22%|██▏       | 77/351 [04:23<15:34,  3.41s/it]\r 22%|██▏       | 78/351 [04:26<15:30,  3.41s/it]\r 23%|██▎       | 79/351 [04:30<15:33,  3.43s/it]\r 23%|██▎       | 80/351 [04:33<15:27,  3.42s/it]\r 23%|██▎       | 81/351 [04:36<15:22,  3.42s/it]\r 23%|██▎       | 82/351 [04:40<15:21,  3.43s/it]\r 24%|██▎       | 83/351 [04:43<15:17,  3.42s/it]\r 24%|██▍       | 84/351 [04:47<15:11,  3.42s/it]\r 24%|██▍       | 85/351 [04:50<15:07,  3.41s/it]\r 25%|██▍       | 86/351 [04:53<15:02,  3.41s/it]\r 25%|██▍       | 87/351 [04:57<15:01,  3.41s/it]\r 25%|██▌       | 88/351 [05:00<15:03,  3.43s/it]\r 25%|██▌       | 89/351 [05:04<14:57,  3.43s/it]\r 26%|██▌       | 90/351 [05:07<14:59,  3.45s/it]\r 26%|██▌       | 91/351 [05:11<14:52,  3.43s/it]\r 26%|██▌       | 92/351 [05:14<14:46,  3.42s/it]\r 26%|██▋       | 93/351 [05:17<14:41,  3.42s/it]\r 27%|██▋       | 94/351 [05:21<14:39,  3.42s/it]\r 27%|██▋       | 95/351 [05:24<14:33,  3.41s/it]\r 27%|██▋       | 96/351 [05:28<14:38,  3.44s/it]\r 28%|██▊       | 97/351 [05:31<14:31,  3.43s/it]\r 28%|██▊       | 98/351 [05:35<14:25,  3.42s/it]\r 28%|██▊       | 99/351 [05:38<14:19,  3.41s/it]\r 28%|██▊       | 100/351 [05:41<14:15,  3.41s/it]\r 29%|██▉       | 101/351 [05:45<14:10,  3.40s/it]\r 29%|██▉       | 102/351 [05:48<14:06,  3.40s/it]\r 29%|██▉       | 103/351 [05:52<14:03,  3.40s/it]\r 30%|██▉       | 104/351 [05:55<13:59,  3.40s/it]\r 30%|██▉       | 105/351 [05:58<13:56,  3.40s/it]\r 30%|███       | 106/351 [06:02<13:55,  3.41s/it]\r 30%|███       | 107/351 [06:05<13:51,  3.41s/it]\r 31%|███       | 108/351 [06:09<13:47,  3.41s/it]\r 31%|███       | 109/351 [06:12<13:43,  3.40s/it]\r 31%|███▏      | 110/351 [06:15<13:39,  3.40s/it]\r 32%|███▏      | 111/351 [06:19<13:36,  3.40s/it]\r 32%|███▏      | 112/351 [06:22<13:32,  3.40s/it]\r 32%|███▏      | 113/351 [06:26<13:28,  3.40s/it]\r 32%|███▏      | 114/351 [06:29<13:25,  3.40s/it]\r 33%|███▎      | 115/351 [06:32<13:21,  3.40s/it]\r 33%|███▎      | 116/351 [06:36<13:18,  3.40s/it]\r 33%|███▎      | 117/351 [06:39<13:18,  3.41s/it]\r 34%|███▎      | 118/351 [06:43<13:17,  3.42s/it]\r 34%|███▍      | 119/351 [06:46<13:12,  3.42s/it]\r 34%|███▍      | 120/351 [06:49<13:11,  3.43s/it]\r 34%|███▍      | 121/351 [06:53<13:12,  3.45s/it]\r 35%|███▍      | 122/351 [06:56<13:05,  3.43s/it]\r 35%|███▌      | 123/351 [07:00<13:06,  3.45s/it]\r 35%|███▌      | 124/351 [07:03<12:59,  3.43s/it]\r 36%|███▌      | 125/351 [07:07<12:53,  3.42s/it]\r 36%|███▌      | 126/351 [07:10<12:51,  3.43s/it]\r 36%|███▌      | 127/351 [07:13<12:46,  3.42s/it]\r 36%|███▋      | 128/351 [07:17<12:41,  3.41s/it]\r 37%|███▋      | 129/351 [07:20<12:36,  3.41s/it]\r 37%|███▋      | 130/351 [07:24<12:32,  3.41s/it]\r 37%|███▋      | 131/351 [07:27<12:28,  3.40s/it]\r 38%|███▊      | 132/351 [07:30<12:24,  3.40s/it]\r 38%|███▊      | 133/351 [07:34<12:21,  3.40s/it]\r 38%|███▊      | 134/351 [07:37<12:17,  3.40s/it]\r 38%|███▊      | 135/351 [07:41<12:15,  3.40s/it]\r 39%|███▊      | 136/351 [07:44<12:14,  3.42s/it]\r 39%|███▉      | 137/351 [07:48<12:10,  3.41s/it]\r 39%|███▉      | 138/351 [07:51<12:05,  3.41s/it]\r 40%|███▉      | 139/351 [07:54<12:11,  3.45s/it]\r 40%|███▉      | 140/351 [07:58<12:04,  3.43s/it]\r 40%|████      | 141/351 [08:01<12:03,  3.44s/it]\r 40%|████      | 142/351 [08:05<11:57,  3.43s/it]\r 41%|████      | 143/351 [08:08<11:51,  3.42s/it]\r 41%|████      | 144/351 [08:12<11:50,  3.43s/it]\r 41%|████▏     | 145/351 [08:15<11:44,  3.42s/it]\r 42%|████▏     | 146/351 [08:18<11:39,  3.41s/it]\r 42%|████▏     | 147/351 [08:22<11:37,  3.42s/it]\r 42%|████▏     | 148/351 [08:25<11:32,  3.41s/it]\r 42%|████▏     | 149/351 [08:29<11:32,  3.43s/it]\r 43%|████▎     | 150/351 [08:32<11:29,  3.43s/it]\r 43%|████▎     | 151/351 [08:36<11:23,  3.42s/it]\r 43%|████▎     | 152/351 [08:39<11:19,  3.41s/it]\r 44%|████▎     | 153/351 [08:42<11:14,  3.41s/it]\r 44%|████▍     | 154/351 [08:46<11:10,  3.40s/it]\r 44%|████▍     | 155/351 [08:49<11:06,  3.40s/it]\r 44%|████▍     | 156/351 [08:53<11:04,  3.41s/it]\r 45%|████▍     | 157/351 [08:56<11:01,  3.41s/it]\r 45%|████▌     | 158/351 [08:59<10:59,  3.42s/it]\r 45%|████▌     | 159/351 [09:03<10:59,  3.44s/it]\r 46%|████▌     | 160/351 [09:06<10:54,  3.43s/it]\r 46%|████▌     | 161/351 [09:10<10:51,  3.43s/it]\r 46%|████▌     | 162/351 [09:13<10:46,  3.42s/it]\r 46%|████▋     | 163/351 [09:16<10:41,  3.41s/it]\r 47%|████▋     | 164/351 [09:20<10:39,  3.42s/it]\r 47%|████▋     | 165/351 [09:23<10:34,  3.41s/it]\r 47%|████▋     | 166/351 [09:27<10:31,  3.41s/it]\r 48%|████▊     | 167/351 [09:30<10:27,  3.41s/it]\r 48%|████▊     | 168/351 [09:34<10:23,  3.40s/it]"
     ]
    }
   ],
   "source": [
    "for idx in range(EPOCHS):\n",
    "    train_loop_fn(train_loader, model, optimizer, DEVICE)\n",
    "    valid_loop_fn(valid_loader, model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loop_fn(valid_loader, model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = CoNLLDataset(graphs=valid_graphs, tokenizer=TOKENIZER, max_len=MAX_LEN, fullvocab=train_dataset._fullvocab)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=4, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, batch in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "\n",
    "        p_logits, f_logits = model(batch['ids'].cuda(), batch['mask'].cuda())\n",
    "\n",
    "        #UPOS\n",
    "        b,s,l = p_logits.size()\n",
    "        p_loss = loss_fn(p_logits.view(b*s,l), batch['upos_ids'].cuda().view(b*s))\n",
    "        p_total_loss.append(p_loss.item())\n",
    "        p_total_pred.extend(torch.argmax(p_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        p_total_targ.extend(batch['upos_ids'].cuda().view(b*s).cpu().tolist())\n",
    "\n",
    "        #FEAT\n",
    "        b,s,l = f_logits.size()\n",
    "        f_loss = loss_fn(f_logits.view(b*s,l), batch['feat_ids'].cuda().view(b*s))\n",
    "        f_total_loss.append(f_loss.item())\n",
    "        f_total_pred.extend(torch.argmax(f_logits.view(b*s,l), 1).cpu().tolist())\n",
    "        f_total_targ.extend(batch['feat_ids'].cuda().view(b*s).cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "f92b6f904071e3223de4cbe10e9c316f933c81f6967af53b0fc43471642e13f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
